{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5 - Midterm 1 Review\n",
    "This notebook is a review notebook. In order to serve the purpose of recalling and internalizing knowledge, the section should involve more discussion than usual, between all people in the class, whether students or TA. Most of the code is written for you. Some blanks are meant to be filled, and you should fill them in yourself or discuss as a group before filling in and running the cell.\n",
    "\n",
    "We will recap:\n",
    "1. Sampling\n",
    "2. Linear Regression\n",
    "3. PCA\n",
    "\n",
    "You should have downloaded:\n",
    "- sampling-data1.csv\n",
    "- sampling-data2.csv\n",
    "- linreg-data.csv\n",
    "- spikes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Linear Regression\n",
    "\n",
    "### Step 1: make augmented X\n",
    "### Step 2: Compute coefficients via magic formula (pseudoinverse)\n",
    "### Step 3: Use the model! \n",
    "\n",
    "First, we load data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('linreg-data.csv', delimiter=',')\n",
    "x = data[0,:]\n",
    "y = data[1,:]\n",
    "\n",
    "plt.scatter(x, y, label='Data', c='b')\n",
    "plt.xlabel('X'); plt.ylabel('Y'); plt.legend(); \n",
    "plt.axis('equal'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: make augmented X\n",
    "The linear regression model is\n",
    "$$ y = a + bX = a \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} + b  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} .$$\n",
    "\n",
    "We can rewrite it as $$ y = X' \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\begin{bmatrix} 1  & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix}, $$ i.e., $X'$ is $X$ with a column of 1s augmented to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a column of ones to x for the bias term\n",
    "X = None    # TODO how should we initialize array?\n",
    "X[:,1] = None                     # TODO what should be in the second column?\n",
    "X                   # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute coefficients via magic formula (pseudoinverse)\n",
    "**Discuss:**\n",
    "- Which is the intercept?\n",
    "- Which is the slope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the coefficients using the pseudoinverse\n",
    "beta = None        # TODO what is this formula?\n",
    "a,b = beta\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use the model! \n",
    "We can:\n",
    "1. predict on test data (plt.scatter)\n",
    "2. plot the best fit line/curve (plt.plot)     <-- we will do this today\n",
    "\n",
    "It is basically the same thing. _Usually_, we visualize it differently by using different plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make augmented matrix for new points\n",
    "x_test = np.linspace(-0.5, 2.5, 100)\n",
    "X_test = np.ones((len(x_test),2))\n",
    "X_test[:,1] = x_test\n",
    "\n",
    "# Create the regression line\n",
    "y_test = None            # TODO what is the formula?\n",
    "\n",
    "# Plot the dataset and regression line\n",
    "plt.scatter(x, y, label='Data', c='b')\n",
    "plt.plot(x_test, y_test, label='Regression Line', c='r')\n",
    "plt.title('Linear Regression'); plt.xlabel('X'); plt.ylabel('Y'); plt.legend(); plt.axis('equal'); \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: Common mistake in homework\n",
    "Append column of 1's in the wrong place (on the right instead of the left)\n",
    "$$ y = X' \\begin{bmatrix} a \\\\ b \\end{bmatrix}  \n",
    "= \\begin{bmatrix} x_1 & \\color{red}{1} \\\\ x_2 & \\color{red}{1} \\\\ \\vdots & \\color{red}{\\vdots}  \\\\ x_n & \\color{red}{1} \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} \n",
    "= a \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + b \\color{red}{\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}} = aX+b.$$\n",
    "\n",
    "\n",
    "But $aX + b  \\neq a + bX$! So this is a different* model. \n",
    "\n",
    "- This ia half a lie. It is a different model, but not tooooo different. If you swap the roles $a=b$ and $b=a$, then you fix the issue. The moral of the story is just be careful about where the 1s column is and how you interpret it. \n",
    "\n",
    "- If you put it on the right instead of left, you still get something \"correct\", just with swapped numbers.\n",
    "\n",
    "We demonstrate this swap below. Run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demostration for a swapping with b, columns are reversed\n",
    "X_swap = X[:,[1,0]]\n",
    "X_swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_swap = np.linalg.pinv(X_swap) @ y\n",
    "a_swap, b_swap = beta_swap\n",
    "\n",
    "# Print the calculated coefficients\n",
    "print('a + bX model (original)')\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\\n\")\n",
    "\n",
    "print('aX + b model (swapped)')\n",
    "print(f\"a: {a_swap}\")\n",
    "print(f\"b: {b_swap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 PCA\n",
    "Let's use the data points above and define new array `X`.\n",
    "\n",
    "**Disucss:**\n",
    "- What are the dimensions of `X`? What are the rows? What are the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Center the data\n",
    "### Step 2: Compute covariance matrix\n",
    "### Step 3: Get eigen information\n",
    "### Step 4: Use the model! (dimensionality reduction, etc.)\n",
    "\n",
    "We can do all of this pretty quickly. \n",
    "- Respond to the questions commented next to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this step doing? Ans: centering data\n",
    "X_ctd = X - X.mean(axis=1, keepdims=True)       \n",
    "\n",
    "# TODO what is the formula? Ans: computing covariance matrix\n",
    "C = None              \n",
    "\n",
    "# what does U represent? what is D? what is the relationship of U and W?\n",
    "U, D, W = np.linalg.svd(C)                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U[:,0] = -U[:,0]      # this is just to make the visualization nicer, it's still an eigenvector\n",
    "\n",
    "# Plot the dataset and regression line\n",
    "plt.scatter(X_ctd[0,:], X_ctd[1,:], c='b')\n",
    "plt.quiver([0,0], [0,0], U[0,:], U[1,:], color='k', angles='xy', scale_units='xy', scale=1, label='PCs')\n",
    "plt.title('PCA'); plt.xlabel('X'); plt.ylabel('Y'); plt.axis('equal'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Projections\n",
    "We learned that the projection is $U_1U_1^TX$, visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = None       # TODO how do we turn the math into code? You may need to do U[:,:1] instead of U[:,0]\n",
    "\n",
    "plt.scatter(X_ctd[0,:], X_ctd[1,:], c='b')\n",
    "plt.scatter(P1[0,:], P1[1,:], c='orange', label='projection')\n",
    "plt.quiver([0,0], [0,0], U[0,:], U[1,:], color='k', angles='xy', scale_units='xy', scale=1, label='PCs')\n",
    "plt.plot(np.vstack((X_ctd[0,:], P1[0,:])), np.vstack((X_ctd[1,:], P1[1,:])), '--', c='orange')\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title('Projecting onto PC1');plt.legend(loc='lower right')\n",
    "plt.axis('equal'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Scree plot\n",
    "**Goal:** decide how many principal components we should keep in dimensionality reduction. \n",
    "\n",
    "We bring back the spikes dataset that we've seen before and use sklearn PCA.\n",
    "\n",
    "**Task:**\n",
    "- Complete the code to\n",
    "    - create pca object\n",
    "    - fit data\n",
    "    - compute eigendecomposition\n",
    "- Tip: package automatically centers data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_spike = np.loadtxt('spikes.csv', delimiter=',')\n",
    "n, m = X_spike.shape\n",
    "\n",
    "# sklearn PCA\n",
    "pca = None                      # TODO create model and fit in one line\n",
    "evals, evecs = None, None     # TODO get evals and evecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate the scree plots.\n",
    "\n",
    "**Discuss:**\n",
    "- What is each plot representing?\n",
    "- What is the units of the y-axis? Why is it that way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained variance plot, in percentage\n",
    "exp_var = (evals / np.sum(evals)) * 100\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "plt.bar(range(1, n+1), exp_var, align='center',label='Individual Explained Variance')\n",
    "plt.step(range(1, n+1), cum_exp_var, where='mid', label='Cumulative Explained Variance', color='red')\n",
    "\n",
    "plt.xlabel('Principal Component Index'); plt.ylabel('Explained Variance %'); plt.legend(loc='best'); \n",
    "plt.xticks(range(0,n+1,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose number of PCs\n",
    "\n",
    "There is no hard-and-fast rule for how many principal components you should choose when reducing the dimensionality.  This is when math/data science becomes more of an art.\n",
    "\n",
    "**Discuss:** \n",
    "1. Look at where the individual explained variance has an \"elbow\". At which principal component index do you see that happening?\n",
    "2. Look at where the cumulative variance exceeds some \"high enough\" value (≈80-90%). At which principal component index do you see that happening?\n",
    "\n",
    "**(For fun) Further discussion:** \n",
    "- In what scenarios would the two approaches for choosing PCs above give the same number? In what situations would they give different numbers?\n",
    "\n",
    "- What other methods do you know/think we can use to choose the number of PCs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Kernel Density Estimate (KDE)\n",
    "**Big idea:** Given some samples $x_1, \\dots, x_n$, we can create a guess for what the actual density looks like. \n",
    "\n",
    "**Discuss:**\n",
    "How does the choice of ________ affect the KDE?\n",
    "1. **kernel function** (e.g., gaussian, triangle, uniform, ...)\n",
    "\n",
    "2. **bandwidth** \n",
    "\n",
    "See the corresponding lecture and section notebook for more details on implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Sampling\n",
    "We consider the problem of inferring whether a sample is from a particular distribution, using histograms. Here are two different approaches:\n",
    "\n",
    "a. sample histogram vs pdf\n",
    "\n",
    "b. sample histogram vs histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Check sample 1 against normal\n",
    "### Approach a. histogram vs pdf\n",
    "The following code loads the samples and plots its histogram against a normal pdf.\n",
    "\n",
    "**Task:**\n",
    "- Complete the code to compute sample mean and unbiased standard deviation `mean1` and `std1`.\n",
    "- Run the cell and respond to the following discussion questions.\n",
    "\n",
    "_(It is recommended to write your responses to the following prompts on paper/black/whiteboard or your paper. We will compare and contrast with future examples.)_\n",
    "\n",
    "**Discuss:**\n",
    "- What difference in **shape** do you see between histogram and normal pdf?\n",
    "- What are the differences between their **tail behaviors**? In particular, what values does the sample take and what does a normal distribution take?\n",
    "- What **other** similarities or differences do you see?\n",
    "- Do you think the samples are from a normal distribution?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.loadtxt('sampling-data1.csv')\n",
    "\n",
    "mean1 = None     # TODO mean\n",
    "std1  = None     # TODO unbiased standard deviation\n",
    "\n",
    "# plot\n",
    "xmin, xmax = -0.5, 1.5\n",
    "bin = np.linspace(xmin,xmax,50)\n",
    "\n",
    "plt.hist(data1, bins=bin, range=(0,1), density=True, alpha=0.6, color='g', edgecolor='black', label='data')\n",
    "plt.plot(np.linspace(xmin,xmax,1000), norm.pdf(np.linspace(xmin,xmax,1000),loc=mean1, scale=std1),'r-', lw=2, label='normal PDF')\n",
    "plt.title('Data versus normal density'); plt.xlabel('Value'); plt.ylabel('Density'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach b. histogram vs histogram\n",
    "**Task:**\n",
    "- Generate the same number of samples from a normal distribution with sample mean and std from above.\n",
    "- Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from normal with sample mean and std\n",
    "ndata = len(data1)\n",
    "data1_prime = None # TODO generate ndata samples from normal(mean,std)\n",
    "\n",
    "# plot\n",
    "plt.hist(data1, bins=bin, density=True, alpha=1, color='g', edgecolor='black', label='original data')\n",
    "plt.hist(data1_prime, bins=bin, density=True, alpha=0.6, color='r', edgecolor='black', label='simulated normal data')\n",
    "plt.title('Data vs normal histogram'); plt.xlabel('Value'); plt.ylabel('Density'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Check sample 2 against normal\n",
    "### Approach a. histogram vs pdf\n",
    "**Task:**\n",
    "- Complete the code to compute sample mean and unbiased standard deviation `mean2` and `std2`.\n",
    "- Run the cell and respond to the following discussion questions.\n",
    "\n",
    "**Discuss:**\n",
    "- Same questions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('sampling-data2.csv')\n",
    "mean2 = None          # TODO mean\n",
    "std2  = None   # TODO unbiased standard deviation\n",
    "\n",
    "# plot\n",
    "xmin, xmax = -4, 4\n",
    "bin = np.linspace(xmin,xmax,50)\n",
    "\n",
    "plt.hist(data2, bins=bin, density=True, alpha=0.6, color='g', edgecolor='black', label='data')\n",
    "plt.plot(np.linspace(xmin,xmax,1000), norm.pdf(np.linspace(xmin,xmax,1000),loc=mean2, scale=std2),'r-', lw=2, label='normal PDF')\n",
    "plt.title('Data and normal density'); plt.xlabel('Value'); plt.ylabel('Density'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach b. histogram vs histogram\n",
    "**Task:**\n",
    "- Generate the same number of samples from a normal distribution with sample mean and std from above.\n",
    "- Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from normal with sample mean and std\n",
    "data2_prime = None\n",
    "\n",
    "# plot\n",
    "plt.hist(data2, bins=bin, density=True, alpha=1, color='g', edgecolor='black', label='original data')\n",
    "plt.hist(data2_prime, bins=bin, density=True, alpha=0.6, color='r', edgecolor='black', label='simulated normal data')\n",
    "plt.title('Data versus normal histogram'); plt.xlabel('Value'); plt.ylabel('Density'); plt.legend(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
