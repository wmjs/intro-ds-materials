{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9e77fe",
   "metadata": {},
   "source": [
    "# Section 9 - Clustering Models: K-Means and Gaussian Mixture Models (GMM)\n",
    "\n",
    "You should have downloaded:\n",
    "- t1_icbm_normal_1mm_pn3_rf20.rawb\n",
    "- utils.py\n",
    "\n",
    "\n",
    "Goals:\n",
    "- Review lecture content\n",
    "- Understand K-Means and GMM via hands-on coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc714f21",
   "metadata": {},
   "source": [
    "## 0 Introduction\n",
    "\n",
    "```t1_icbm_normal_1mm_pn3_rf20.rawb``` contains MR images of cross-sections of the brain.\n",
    "\n",
    "The collection of images is defined as a 3D tensor of shape (H, W, C), where \n",
    "- H = height of image\n",
    "- W = width of image\n",
    "- C = number of cross-sections scanned\n",
    "\n",
    "Moreover, images are represented on a gray scale, i.e., each image has only one color channel with pixel values in the range ```[0, 255]```.\n",
    "\n",
    "**Goal:** based on the MRI, segment/cluster regions of the brain that seem to be similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a8f93-f52c-4d17-b7b2-91f9dea1b7c7",
   "metadata": {},
   "source": [
    "## 1 Warm-up exercise\n",
    "\n",
    "Load the MRI Brain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f778d4c-1ac7-4d0e-ba68-b7527c8bb546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Image_raw = np.fromfile(\"t1_icbm_normal_1mm_pn3_rf20.rawb\",  dtype=np.uint8)\n",
    "Image = np.reshape(Image_raw,(181,217,181),order='F') # hard coded dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c8a5a",
   "metadata": {},
   "source": [
    "### 1.1 Heuristic Clustering\n",
    "**Task:**\n",
    "\n",
    "Plot the **80th cross section** using `plot_cross_sect_with_threshold`, which is imported from a separate file. \n",
    "\n",
    "Experiment with a range of specified ranges:\n",
    "- threshold1=80 and threshold2=160 (split into roughly equal thirds)\n",
    "- threshold1=80 and threshold2=140 (keep first third, but lower threshold2 )\n",
    "- threshold1=70 and threshold2=120 (yet another similar option, which will make more sense when you see the histogram later)\n",
    "\n",
    "**Discuss:**\n",
    "- How did the thresholds affect the plots? Were some better than others in visualizing the brain regions that look similar?\n",
    "- Imagine the thresholds were not given to you as above, how would you go about efficiently segmenting the brain regions? Can you automate it? How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_cross_sect_with_threshold\n",
    "\n",
    "plot_cross_sect_with_threshold(img=None, threshold1=None, threshold2=None)        # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10f76dd6-28e2-4c31-8388-55541cc2e210",
   "metadata": {},
   "source": [
    "### 1.2 Histogram\n",
    "To see why the third option (threshold1=70 and threshold2=120) gave the most convincing segmentation, take a look at the histogram of pixel values over all cross sections.\n",
    "\n",
    "**Key point:** it is tedious to have to *manually* do these segmentation/clustering. That is one reason why we want to \"automate\" it through clustering algorithms, such as k-means and gaussian mixture models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299df37-228b-4063-b5fa-58cae121d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = plt.hist(Image.flatten(), bins=255)\n",
    "plt.title(\"Histogram of pixel values across all cross sections\")\n",
    "plt.xlabel('pixel value'); plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472299f",
   "metadata": {},
   "source": [
    "## 2 K-means\n",
    "\n",
    "### 2.1 Recap\n",
    "*Adapted from: Wikipedia <a href=https://en.wikipedia.org/wiki/K-means_clustering>K-means Clustering</a>*\n",
    "\n",
    "K-Means is a method used to partition data into $k$ cluster/groups of similar observations. \n",
    "\n",
    "### 2.2 Training\n",
    "- **Centroid.** Each cluster i has a center, a.k.a. centroid $\\mu_i$.\n",
    "- **How to compute centroid.** Given a partition of the _training_ data into $k$ clusters $G = (G_1, G_2, \\dotsi, G_k)$, the centroid is simply the average location of all points in the same cluster.\n",
    "    - The k-means clusters is the **optimal partition that minimizes the sum of (squared) distances to the centroids**.\n",
    "    $$\\argmin_{G} \\sum_{i=1}^k \\sum_{x \\in G_i} ||x - \\mu_{i}||^2$$\n",
    "    - In other words, the goal of k-means is to minimize the within-cluster variance. \n",
    "\n",
    "#### Iterative method for training\n",
    "**Task:** fill in the blanks\n",
    "1. **Initialize centroid.** Choose initial set of __________ $(\\mu_1, \\dots, \\mu_k)$. Often, we just randomly pick $k$ data points as the initial centroids.\n",
    "\n",
    "2. **Assign points.** For each training point, assign it to __________ cluster centroid.\n",
    "<!-- $$\\displaystyle G_{i}^{(t)} = \\{x_j: ||x_m - \\mu_{i}^{(t)}||^2 \\leq ||x_m - \\mu_{j}^{(t)}||^2, \\ j = 1, \\dotsi, k\\}$$ -->\n",
    "\n",
    "3. **Update centroid.** Recalculate each cluster's centroid as the __________ of all points assigned to the cluster.\n",
    "<!-- >$\\displaystyle \\mu_{i}^{(t+1)} = \\dfrac{1}{|G_{i}^{(t)}|}\\sum_{x \\in G_{i}^{(t)}}x_i$ -->\n",
    "\n",
    "4. **Repeat until __________ or __________.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb587f53",
   "metadata": {},
   "source": [
    "### 2.3 Hands-on implementation\n",
    "**Task:** complete the code below for implementing k-means.\n",
    "- `initialize` has been done for you and is imported from a separate file.\n",
    "- `assign`\n",
    "- `update`\n",
    "- `my_kmeans`, which combines all steps of k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3249e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import initialize\n",
    "\n",
    "def assign(X, centroids):\n",
    "    \"\"\"\n",
    "    Assign data points to the nearest cluster centroids.\n",
    "\n",
    "    Parameters:\n",
    "        X        (1D numpy array): MRI voxels\n",
    "        centroids(1D numpy array): Current cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "        clusters (1D numpy array): Cluster assignments for each data point.\n",
    "    \"\"\"\n",
    "    # Calculate the  distance between data points and centroids.\n",
    "    dist = None             # TODO\n",
    "\n",
    "    # Assign each data point to the cluster with the nearest centroid.\n",
    "    clusters = None          # TODO\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def update(X, clusters, num_cluster):\n",
    "    \"\"\"\n",
    "    Update cluster centroids based on the assigned data points.\n",
    "\n",
    "    Parameters:\n",
    "        X        (1D numpy array): MRI voxels\n",
    "        clusters (1D numpy array): Cluster assignments for each data point.\n",
    "        num_cluster         (int): Number of clusters.\n",
    "\n",
    "    Returns:\n",
    "        new_centroids (1D numpy array): Updated cluster centroids.\n",
    "    \"\"\"\n",
    "    new_centroids = np.zeros(num_cluster)\n",
    "    \n",
    "    # Iterate through each cluster.\n",
    "    for k in range(num_cluster):\n",
    "        # Calculate the mean of data points in the current cluster and update the centroid.\n",
    "        new_centroids[k] = None         # TODO\n",
    "\n",
    "    return new_centroids\n",
    "\n",
    "\n",
    "def my_kmeans(X, num_cluster):\n",
    "    \"\"\"\n",
    "    Perform the K-Means clustering algorithm on input data.\n",
    "\n",
    "    Parameters:\n",
    "        X (1D numpy array): MRI voxels\n",
    "        num_cluster  (int): Number of clusters.\n",
    "\n",
    "    Returns:\n",
    "        clusters  (1D numpy array): Cluster assignments for each data point.\n",
    "        centroids (1D numpy array): cluster centroids.\n",
    "    \"\"\"\n",
    "    iter, max_iter = 0, 10\n",
    "\n",
    "    centroids, _, _ = initialize(X, num_cluster)\n",
    "    clusters = assign(X, centroids)\n",
    "    while iter <= max_iter:\n",
    "        centroids = None       # TODO\n",
    "        clusters  = None       # TODO\n",
    "        iter += 1\n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24748e9",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "- run kmeans on the full image to segment the image into 4 clusters.\n",
    "- plot the clusters on the 80th cross section image. Compare it against the manual thresholding you did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ravel(Image)\n",
    "\n",
    "clustered_X_kmeans, centroids = None            # TODO\n",
    "clustered_image_kmeans = np.reshape(clustered_X_kmeans, (181,217,181))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0580975",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_idx = 80\n",
    "from utils import plot_original_image, plot_segmented_image\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1,2,1)\n",
    "plot_original_image(Image, slice_idx)\n",
    "\n",
    "# Clustered image\n",
    "plt.subplot(1,2,2)\n",
    "plot_segmented_image(clustered_image_kmeans, slice_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc870dd4-6c89-430e-a748-507e7c41b552",
   "metadata": {},
   "source": [
    "## 3 Gaussian Mixture Model (GMM)\n",
    "### 3.1 Recap\n",
    "*Adapted from: Matt Bonakdarpour, <a href=https://stephens999.github.io/fiveMinuteStats/intro_to_em.html>\"Introduction to EM: Gaussian Mixture Models.\"</a>*\n",
    "\n",
    "\n",
    "A Gaussian Mixture Model is a probabilistic model that represents the data as a mixture of $k$ Gaussian distributions, each with unknown parameters $\\theta$. \n",
    "\n",
    "For example, suppose you have a dataset and you assume that it can be modeled as a mixture of two Gaussians ($k = 2$). Then, your model would have the following parameters:\n",
    "- $(\\pi_1, \\mu_1,\\sigma_1)$\n",
    "- $(\\pi_2, \\mu_2,\\sigma_2)$ \n",
    "\n",
    "where $\\pi_{k}, \\mu_{k}, \\sigma_{k}$ are the prior, the mean and the variance for each Gaussian, and $\\pi_1\\!+\\!\\pi_2\\!=\\!1$. plus latent cluster assigment variables (explained below).\n",
    "\n",
    "### 3.2 Training\n",
    "**Goal:** find the set of parameters that **maximize the likelihood function**. \n",
    "\n",
    "Assuming our observations $x_1, \\dots, x_n$ are i.i.d., the likelihood function is:\n",
    "$$L(\\theta;x) = \\prod_{i=1}^n \\Big[\\pi_1\\, \\mathcal{N}\\left(x_i;\\mu_1,\\sigma_1\\right) + \\pi_2 \\, \\mathcal{N}\\left(x_i; \\mu_2,\\sigma_2\\right)\\Big],$$\n",
    "which we can simplify further by applying a *log* to change products into sums\n",
    "$${\\color{red}{\\log}} \\ L(\\theta;x) = {\\color{red}{\\sum_{i=1}^n \\log}}\\Big[\\pi_1\\, \\mathcal{N}\\left(x_i;\\mu_1,\\sigma_1\\right) + \\pi_2 \\, \\mathcal{N}\\left(x_i; \\mu_2,\\sigma_2\\right)\\Big].$$\n",
    "\n",
    "#### Iterative method for training\n",
    "- However, maximizing the log-likelihood function does not have a closed form/analytic solution\n",
    "- we need to make use of an iterative method to optimize the parameters. \n",
    "- For GMMs, the go to method is the **Expectation-Maximization Algorithm (EM)**.\n",
    "\n",
    "\n",
    "**Task:** fill in the blanks\n",
    "1. **Initialize.** Choose initial __________, can be done randomly or using result from k-means.\n",
    "2. **Update (EM).** Compute a better $\\theta'$ s.t. the __________ improves: $\\log L(\\theta';x) > \\log L(\\theta;x)$. (Can also swap likelihood for posterior.)\n",
    "3. **Repeat until __________ or __________.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b9462",
   "metadata": {},
   "source": [
    "### 3.3 How Update (EM) works\n",
    "#### 1. E-step \n",
    "Define a **new random variable $Z_i$**, which represents the cluster assigment of data point $i$\n",
    "- So, ${\\color{red}{P(Z_i = k | x_i)}}$ is the posterior probability that data point $i$ belongs to cluster $k$, given the specific observed value $x_i$.\n",
    "- **Bayes rule** to compute $P(Z_i = k | x_i)$ with current parameters $\\mu_k, \\sigma_k, \\pi_k$\n",
    "$$ {\\color{red}{P(Z_i = k | x_i)}} \\ = \\ \\frac{{\\color{blue}{P(x_i | Z_i = k)}}{\\color{orange}{P(Z_i = k)}}}{P(x_i)} \\ = \\ \\frac{{\\color{orange}{\\pi_k}} {\\color{blue}{\\mathcal{N}(x_i;\\mu_k, \\sigma_{k})}}}{\\sum_{k=1}^{K}\\pi_k \\mathcal{N}(x_i;\\mu_k, \\sigma_{k})}$$\n",
    "\n",
    "#### 2. M-step\n",
    "**Update parameters**\n",
    "\n",
    "For convenience, **define $N_k$** (which can be thought of as \"the number of points assigned to cluster $k$\")\n",
    "$$ N_k := \\sum_{i=1}^n {\\color{red}{P(Z_i = k | x_i)}}.$$\n",
    "The updates are:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mu'_k    &= \\dfrac{1}{N_k} \\sum_{i=1}^n {{x_i}} {\\color{red}{P(Z_i = k | x_i)}}\\\\\n",
    "    {\\sigma^2_k}' &= \\dfrac{1}{N_k}\\sum_{i=1}^n {{(x_i - \\mu_k)^2}} {\\color{red}{P(Z_i = k | x_i)}}\\\\\n",
    "    \\pi'_k    &= \\dfrac{N_k}{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### 3. Repeat if necessary\n",
    "**Check.** Evaluate log likelihood using new parameters $\\mu'_k, {\\sigma_k}', \\pi'_k$.\n",
    "- Stop if:\n",
    "    - converges (i.e., log-likelihood has only increases by less than some small $\\epsilon$)\n",
    "    - max iterations reached\n",
    "- Otherwise, repeat steps E and M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b2cea",
   "metadata": {},
   "source": [
    "### 3.4 Hands-on Implementation\n",
    "In the code below, we use `gamma[i,k]` to represent $P(Z_i = k | x_i)$.\n",
    "\n",
    "**Task:** complete the code below to implement the EM algorithm and Gaussian Mixture Models for clustering.\n",
    "- `expectation`\n",
    "- `maximization`\n",
    "- `log_likelihood`, which has been done for you\n",
    "- `my_GMM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from utils import plot_gmm_steps\n",
    "\n",
    "\n",
    "def expectation(X, mu, var, prior):\n",
    "    \"\"\"\n",
    "    Estimates the cluster membership probabilities (posterior)\n",
    "      for each voxel* in a 3D MRI volume.\n",
    "    (*: Voxel is the analogue of a pixel in 3D.)\n",
    "\n",
    "    Parameters:\n",
    "        X     (1D numpy array): MRI voxels\n",
    "        mu    (1D numpy array): cluster means\n",
    "        var   (1D numpy array): cluster variances\n",
    "        prior (1D numpy array): cluster prior probabilities\n",
    "\n",
    "    Returns:\n",
    "        gamma (2D numpy array: num_voxels x num_clusters): \n",
    "            cluster membership probabilities for each voxel \n",
    "    \"\"\"\n",
    "    num_clusters = len(mu)\n",
    "    gamma = np.zeros((len(X), num_clusters))\n",
    "    normalizing_constant = np.zeros(len(X))\n",
    "\n",
    "    # Calculate unnormalized cluster probabilities\n",
    "    for k in range(num_clusters):\n",
    "        norm_dist  = None           # TODO\n",
    "        gamma[:,k] = None           # TODO\n",
    "        normalizing_constant += gamma[:,k]\n",
    "\n",
    "    # Normalize probabilities\n",
    "    for k in range(num_clusters):\n",
    "        gamma[:,k] /= None          # TODO\n",
    "\n",
    "    return gamma\n",
    "\n",
    "def maximization(X, gamma):\n",
    "    \"\"\"\n",
    "    Updates the cluster mean, variance, and prior probability according to the latest class membership.\n",
    "\n",
    "    Parameters:\n",
    "        X     (1D numpy array): MRI voxels\n",
    "        gamma (2D numpy array: num_voxels x num_clusters): \n",
    "            cluster membership probabilities for each voxel  \n",
    "\n",
    "    Returns:\n",
    "        mu    (1D numpy array): updated cluster means\n",
    "        var   (1D numpy array): updated cluster variances\n",
    "        prior (1D numpy array): updated cluster prior probabilities\n",
    "    \"\"\"\n",
    "    N_k = None          # TODO\n",
    "    # mu\n",
    "    mu  = None          # TODO\n",
    "    # var\n",
    "    var = None          # TODO\n",
    "    # prior\n",
    "    prior = None        # TODO\n",
    "\n",
    "    return mu, var, prior\n",
    "\n",
    "def log_likelihood(X, mu, var, prior):\n",
    "    \"\"\"\n",
    "    Estimates the total log-likelihood for the image labeling.\n",
    "\n",
    "    Parameters:\n",
    "        X     (1D numpy array): MRI voxels\n",
    "        mu    (1D numpy array): cluster means\n",
    "        var   (1D numpy array): cluster variances\n",
    "        prior (1D numpy array): cluster prior probabilities\n",
    "\n",
    "    Returns:\n",
    "        log_likelihood (float)\n",
    "    \"\"\"\n",
    "    num_clusters = len(mu)\n",
    "    likelihood_terms = np.zeros(len(X))\n",
    "\n",
    "    for k in range(num_clusters):\n",
    "        norm_dist = norm.pdf(X, loc=mu[k], scale=np.sqrt(var[k]))\n",
    "        likelihood_terms += prior[k] * norm_dist\n",
    "\n",
    "    log_likelihood = np.sum(np.log(likelihood_terms))\n",
    "    return log_likelihood\n",
    "\n",
    "def my_GMM(X, n_cluster):\n",
    "    \"\"\"\n",
    "    Run the Gaussian Mixture Model on the image data.\n",
    "\n",
    "    Parameters:\n",
    "        X    (1D numpy array): MRI voxels\n",
    "        n_cluster   (integer): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "        mu    (1D numpy array): cluster means\n",
    "        var   (1D numpy array): cluster variances\n",
    "        prior (1D numpy array): cluster prior probabilities\n",
    "        clustered_X (1D numpy array): cluster for each voxel, based on largest likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    iter, max_iter = 0, 8\n",
    "    error = []\n",
    "\n",
    "    mu, var, prior = initialize(X, n_cluster)\n",
    "    while iter < max_iter:\n",
    "        # EM steps\n",
    "        gamma = None              # TODO\n",
    "        mu, var, prior = None     # TODO\n",
    "\n",
    "        # check likelihood\n",
    "        cost = None               # TODO\n",
    "        error.append(cost)\n",
    "\n",
    "        # plot\n",
    "        plot_gmm_steps(X, mu, var, prior, error, max_iter)\n",
    "        \n",
    "        iter += 1\n",
    "\n",
    "    # choose best cluster and sort for convenience\n",
    "    clustered_X = np.argmax(gamma, axis=1)\n",
    "    sort_ind = np.argsort(mu)\n",
    "    return clustered_X, mu[sort_ind], var[sort_ind], prior[sort_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1e2a3",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "- run gmm on the full image to segment the image into 4 clusters.\n",
    "- plot the clusters on the 80th cross section image. Compare it against the manual thresholding you did earlier.\n",
    "- Try num_clusters = 4, 5, 6. What differences do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "clustered_X_gmm, mu, var, cluster_prior = None      # TODO\n",
    "clustered_image_gmm = np.reshape(clustered_X_gmm,(181,217,181))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_idx = 80\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1,2,1)\n",
    "plot_original_image(Image, slice_idx)\n",
    "\n",
    "# Clustered image\n",
    "plt.subplot(1,2,2)\n",
    "plot_segmented_image(clustered_image_gmm, slice_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ce7e",
   "metadata": {},
   "source": [
    "#### 3.5 (Optional) Comparison against sklearn implementation\n",
    "Note: cluster labels may look different, but they cuold just be a reordering of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# sklearn\n",
    "gmm = GaussianMixture(n_components=6)\n",
    "gmm.fit(X.reshape(-1,1))\n",
    "Cpred = gmm.predict(X.reshape(-1,1))\n",
    "\n",
    "# reshape for plotting\n",
    "clustered_image_gmm_sklearn = np.reshape(Cpred, (181,217,181))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a43fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_idx = 80\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1,2,1)\n",
    "plot_original_image(Image, slice_idx)\n",
    "\n",
    "# Clustered image\n",
    "plt.subplot(1,2,2)\n",
    "plot_segmented_image(clustered_image_gmm_sklearn, slice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68944a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "43129c23d79667d987760d8fda822d6cf9b94e4f6ff31aa29025e95d3c53fe91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
