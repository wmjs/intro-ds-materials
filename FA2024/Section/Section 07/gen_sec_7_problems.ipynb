{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defa8d08",
   "metadata": {},
   "source": [
    "# Section 7 - Classification Models: Naive Bayes, LDA, QDA\n",
    "\n",
    "Goals:\n",
    "\n",
    "- Review lecture content on classification methods;\n",
    "- Better understand Naive Bayes, Linear and Quadratic Discriminant Analysis (LDA and QDA) models;\n",
    "- Get a practical sense on model assessment (hypothesis and performance).\n",
    "\n",
    "You should have downloaded:\n",
    "- heart.csv\n",
    "- gnb-lda-qda.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81276f2",
   "metadata": {},
   "source": [
    "# 1: Preprocessing\n",
    "\n",
    "For this section we will use the [Heart Failure Clinical Records Dataset](https://archive.ics.uci.edu/ml/datasets/Heart%2Bfailure%2Bclinical%2Brecords). This dataset contains the medical records of patients who had heart failure, collected during their follow-up period. Each patient profile has 13 clinical features, followed by a label describing if the patient survived or not.\n",
    "\n",
    "**Task:**\n",
    "- Load the [Heart Failure Clinical Records Dataset](https://archive.ics.uci.edu/ml/datasets/Heart%2Bfailure%2Bclinical%2Brecords) from `heart.csv`\n",
    "- Store the following quantitative variables as predictors:\n",
    "    1. `age`\n",
    "    2. `creatinine_phosphokinase`\n",
    "    3. `ejection_fraction`\n",
    "    4. `platelets`\n",
    "    5. `serum_creatinine`\n",
    "    6. `serum_sodium`\n",
    "    - Use a log transformation on the predictors to make them look more like a Gaussian R.V. \n",
    "- Define `y` as the column `DEATH_EVENT` of the dataset. This is the target we want to eventually predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf95feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "predictors = [\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\"]\n",
    "X = dataset[predictors]\n",
    "X = np.log(X)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[\"DEATH_EVENT\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479c0ac",
   "metadata": {},
   "source": [
    "Split the data into training and testing\n",
    "- test_size = 1/3\n",
    "- random_state = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test  = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test  = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769abb20",
   "metadata": {},
   "source": [
    "# 2: Model Fitting\n",
    "GNB, LDA, QDA's models all look something like finding the best posterior amongst the classes $C_k$ (in our case $k=0,1$ for death events)\n",
    "$$\n",
    "\\text{posterior for class k} \\ = \\ P(C_k \\lvert\\,\\boldsymbol{x}) \\  = \\ \\frac{\\pi(C_k)\\,{\\color{red}{{\\cal{}L}_{\\!\\boldsymbol{x}}(C_k)}}}{Z}.\n",
    "$$\n",
    "The key is in the likelihood ${\\color{red}{{\\cal{}L}_{\\!\\boldsymbol{x}}(C_k)}}$.\n",
    "$$\n",
    "\\text{GNB: }  {\\cal{}N} \\left( \\mu_k, {\\color{orange}{D}} \\right), \\quad \\text{LDA: }  {\\cal{}N} \\left( \\mu_k, {\\color{orange}{\\Sigma}} \\right), \\quad \\text{QDA: }  {\\cal{}N} \\left( \\mu_k, {\\color{orange}{\\Sigma_k}} \\right).\n",
    "$$\n",
    "The models are best suited to the following types of data, in increasing complexity.\n",
    "\n",
    "![](gnb-lda-qda.png)\n",
    "\n",
    "$$ \n",
    "GNB: \\ \n",
    "D_0 = \\begin{pmatrix} 1&0\\\\0&1\\end{pmatrix},\n",
    "\\\n",
    "D_1 = \\begin{pmatrix} 3&0\\\\0&0.5\\end{pmatrix}\n",
    "\\qquad\n",
    "LDA: \\ \n",
    "\\Sigma = \\begin{pmatrix} 2&0.7\\\\0.7&1\\end{pmatrix}\n",
    "\\qquad\n",
    "QDA: \\\n",
    "\\Sigma_0 = \\begin{pmatrix} 2&0.7\\\\0.7&1\\end{pmatrix},\n",
    "\\\n",
    "\\Sigma_1 = \\begin{pmatrix} 1&-0.5\\\\-0.5&1\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9236536",
   "metadata": {},
   "source": [
    "**Discuss:**\n",
    "\n",
    "What are the essential differences between the implementation GNB, LDA, and QDA? Write them out below.\n",
    "\n",
    "1. prior calculation? \n",
    "\n",
    "    **Ans:**\n",
    "    \n",
    "2. likelihood calculation?\n",
    "\n",
    "    **Ans:**\n",
    "\n",
    "3. posterior calculation? \n",
    "\n",
    "    **Ans:**\n",
    "\n",
    "4. number of parameters needed to describe the model (complexity)? (Let $K$ be the number of classes, $d$ be dimension of data.)\n",
    "\n",
    "    **Ans:**\n",
    "    \n",
    "**Task:**\n",
    "\n",
    "Create a prediction function `predict()` that implements all models. \n",
    "- Fit the prior and likelihood to the training data\n",
    "    - Note: Infer prior probabilities from class proportions\n",
    "- evaluate likelihood at test points\n",
    "- Use posterior to predict `DEATH_EVENT` at test points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c6b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def predict(X_train, y_train, X_test, model):\n",
    "    # prior\n",
    "    prior0 = None\n",
    "    prior1 = None\n",
    "\n",
    "    # likelihood for each class\n",
    "    X0 = X_train.iloc[y_train[y_train==0].index, :]\n",
    "    X1 = X_train.iloc[y_train[y_train==1].index, :]\n",
    "    \n",
    "    mu0 = None\n",
    "    mu1 = None\n",
    "    \n",
    "    if model == 'gnb':\n",
    "        Sigma0 = None\n",
    "        Sigma1 = None\n",
    "        # print(Sigma0)  # note the dimensions of Sigma0\n",
    "    elif model == 'lda':\n",
    "        Sigma0 = None\n",
    "        Sigma1 = None\n",
    "    elif model == 'qda':\n",
    "        Sigma0 = None\n",
    "        Sigma1 = None\n",
    "\n",
    "    likelihood0 = None\n",
    "    likelihood1 = None\n",
    "\n",
    "    # posterior\n",
    "    # Since we want to predict the class label, we can ignore\n",
    "    # the normalization factor. Just select the one with greatest\n",
    "    # unnormalized posterior.\n",
    "    posterior0 = None\n",
    "    posterior1 = None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61367487",
   "metadata": {},
   "source": [
    "# 3: Model Asessment\n",
    "## 3.1 Accuracy\n",
    "For each model, print the accuracy on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, true_y):\n",
    "    n = pred_y.shape[0]\n",
    "    return 100*np.sum(pred_y == true_y) / n\n",
    "\n",
    "gnb_acc = None\n",
    "lda_acc = None\n",
    "qda_acc = None\n",
    "\n",
    "print(f\"Acc. GNB Model: {np.round(gnb_acc, 2)}%\")\n",
    "print(f\"Acc. LDA Model: {np.round(lda_acc, 2)}%\")\n",
    "print(f\"Acc. QDA Model: {np.round(qda_acc, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf05097",
   "metadata": {},
   "source": [
    "## 3.2 Is data approapriate for models? Check assumptions.\n",
    "### 3.2.1 Check independence/covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data corresponding to each class\n",
    "X0 = X.iloc[y[y==0].index, :]\n",
    "X1 = X.iloc[y[y==1].index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20346",
   "metadata": {},
   "source": [
    "#### GNB\n",
    "- Predictors are **independent**.\n",
    "\n",
    "**Discuss:** \n",
    "1. Independence implies no correlation. Does no correlation imply independence? \n",
    "    \n",
    "    **Ans:** \n",
    "\n",
    "2. How can we check/gauge independence using correlation?\n",
    "\n",
    "    **Ans:** \n",
    "\n",
    "3. By calculating the correlation matrix, what does it suggest about (in)dependence of predictors?\n",
    "\n",
    "    **Ans:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebdcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB: Check the correlation matrix of the predictors within each class.\n",
    "# You may use the pandas.dataframe.corr() function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b5640c",
   "metadata": {},
   "source": [
    "#### LDA\n",
    "- Predictors are **not necessarily independent**, but it is assumed that the **covariance matrix is the same** for each class.\n",
    "\n",
    "#### QDA\n",
    "- Predictors are **not necessarily independent**, and the **covariance matrix is not neccessarily the same** for each class.\n",
    "\n",
    "**Discuss:**\n",
    "- What are some ways to compare close-ness of covariance matrices? Is simply computing them and checking if each entry is exactly the same a fair comparison? What kind of tolerance seems appropriate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA: Check the covariance matrix within each class and see if they are the same.\n",
    "# You may use the pandas.dataframe.cov() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef98e9b",
   "metadata": {},
   "source": [
    "**Discuss:**\n",
    "Based on all the checks we did, how do you make sense of the accuracy results in 3.1 between GNB, LDA, QDA?\n",
    "\n",
    "**Ans:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b99f5",
   "metadata": {},
   "source": [
    "### 3.2.2 Check if data normally distributed\n",
    "Run the code cells below. \n",
    "\n",
    "**Discuss:**\n",
    "- What does each plot represent? How is it computed?\n",
    "- Does the data look normally distributed? \n",
    "- How does that affect the appropriateness of using GNB/LDA/QDA? What are some reasons for or against using them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB, LDA, and QDA: Check if the predictors follow a Gaussian distribution within each class.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots(2,3, figsize=(12,8))\n",
    "for i in range(X0.shape[1]):\n",
    "    row, col = i%2, i%3\n",
    "    ax[row, col].hist(X0.iloc[:, i])\n",
    "    ax[row, col].set_title(f\"Class 0: {X0.columns[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aab86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2,3, figsize=(12,8))\n",
    "for i in range(X1.shape[1]):\n",
    "    row, col = i%2, i%3\n",
    "    ax[row, col].hist(X1.iloc[:, i])\n",
    "    ax[row, col].set_title(f\"Class 1: {X1.columns[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a93c77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
