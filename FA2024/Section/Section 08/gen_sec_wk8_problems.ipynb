{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74980ae9",
   "metadata": {},
   "source": [
    "# Section 8 - Pipeline, Grid Search, Random Forests\n",
    "This section will get you to practice:\n",
    "1. classification algorithms you recently learned in lectures, such as, decision trees, random forests. \n",
    "2. parameter optimization via pipeline and grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a9188-164f-4a81-b690-361e52cd9fce",
   "metadata": {},
   "source": [
    "## 0 Data\n",
    "### Load\n",
    "The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is available from sklearn.datasets.\n",
    "\n",
    "- A summary of information is provided here:\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset\n",
    "- Dataset can also be downloaded from: https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic. \n",
    "\n",
    "**Task:**\n",
    "\n",
    "- Run the cell below to load the breast cancer data \n",
    "- Features `X` consisting of 30 features \n",
    "- target `y`: 0 (benign/harmless/good) and 1 (malignant/harmful/bad). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9101310-dd3f-45b3-9b43-9481619c4363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the Breast Cancer dataset as panda data frame\n",
    "# X stores sample features and y stores labels [0 or 1]\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num of 1s (malignant):  ', np.count_nonzero(y), 'of', len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a2e11-f914-47c9-8153-e87f97e94a16",
   "metadata": {},
   "source": [
    "### 0.2 Split\n",
    "**Discuss:**\n",
    "- What does stratified shuffle split do? What inputs does it take and what outputs does it give to the user?\n",
    "\n",
    "    **Ans:** \n",
    "\n",
    "**Task:**\n",
    "- Split the data into training and test sets using [`sklearn.model_selection.StratifiedShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html). \n",
    "    - Set n_split=1, test_size=1/5, random_state=0.\n",
    "- Verify that the stratified split was performed correctly by printing relevant sizes of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3396850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# use random state to ensure reproducibility (each time we execute this data we get same set\n",
    "# of training and test data\n",
    "sss = None      # TODO\n",
    "for i, (train_idx, test_idx) in enumerate(sss.split(X, y)):\n",
    "    X_train, y_train = None, None      # TODO\n",
    "    X_test , y_test  = None, None      # TODO\n",
    "\n",
    "# Verify correctness of split.\n",
    "print('num of 1 (malignant)')\n",
    "print(\"Training set:\", (y_train==1).sum(), \"out of\", y_train.shape[0])\n",
    "print(\"Test set    :\", (y_test==1).sum(), \"out of\", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3518c2",
   "metadata": {},
   "source": [
    "## 1 Decision Tree GRidSearchCV\n",
    "This step-by-step example is shown before implementing for all others later. The goal is to understand full how gridsearch works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec96c52",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Decision Tree Classifier and Grid Search\n",
    "**Task:**\n",
    "- Create a sklearn Decision Tree classifier `dt_clf`\n",
    "- define `dt_grid`, a dictionary of parameters with the possible values they are allowed to take in the grid search\n",
    "    - criterion: gini or entropy\n",
    "    - max_depth: 2, 3, 4\n",
    "    - min_samples_split: 5, 10, 15?\n",
    "    - **Discuss:** what do each of these parameters mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = None       # TODO\n",
    "\n",
    "# parameters and their possible values\n",
    "dt_grid = {\n",
    "    'criterion': None,          # TODO\n",
    "    'max_depth': None,          # TODO\n",
    "    'min_samples_split': None   # TODO\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c02c28",
   "metadata": {},
   "source": [
    "### Step 2: Find best parameters using GridSearchCV\n",
    "Perform grid search using cross-validation in sklearn.\n",
    "\n",
    "**Task:**\n",
    "- Define `grid_search`, a GridSearchCV object\n",
    "    - input classifier clf and param_grid\n",
    "    - set cv=5, for 5 fold cross validation. Note: these folds are different\n",
    "- fit grid_search to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_search = None            # TODO\n",
    "dt_search.fit(None, None)   # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fb323",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "- extract best parameters (best_params_) and print it\n",
    "- extract best model (best_estimator_) and score its accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None      # TODO\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_estimator = None   # TODO\n",
    "accuracy = best_estimator.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e493e",
   "metadata": {},
   "source": [
    "### Step 3: Visualize best Decision Tree Classifier\n",
    "**Task:**\n",
    "- Visualize your tree result using sklearn.tree plot_tree() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65835025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(90,70))\n",
    "plot_tree(None, feature_names=None, class_names=None, filled=True)  # TODO\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa47cee",
   "metadata": {},
   "source": [
    "## 2 More Classifiers with GridSearchCV\n",
    "### 2.1 Train models\n",
    "We have covered the following classifiers in lecture:\n",
    "- kNN\n",
    "- LDA, QDA, GNB\n",
    "- decision trees, random forests\n",
    "\n",
    "The next exercise is to code up a pipeline that will compare all models at the same time.\n",
    "\n",
    "**Task:**\n",
    "1. Write the function `best_model` which find the best model, given a pipeline/classifier and parameter grid.\n",
    "2. Use [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to find the best classifiers, while allowing a range of parameters to be considered for each classifier. \n",
    "    - \"best\" means the highest accuracy on cross validation.\n",
    "    - You should fit the models to the training data.\n",
    "    - Parameters of each classifier are set with the ‘__’ convention. Look at lecture notes or sklearn documentation for examples.\n",
    "    - Where necessary, seek optimal hyperparameters using `best_model` you wrote above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affe3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Return best classifier from grid search. CV = cross-validation\n",
    "def best_model(pipe, grid, X_train, y_train):\n",
    "    '''\n",
    "    pipe: pipeline object or sklearn classifier object\n",
    "    grid: dictionary of parameters to explore. if using pipeline, \n",
    "            ensure the double underscore __ convention is used\n",
    "    X_train, y_train: the training data\n",
    "    '''\n",
    "    search = None               # TODO\n",
    "                                # TODO\n",
    "    return None                 # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece27955",
   "metadata": {},
   "source": [
    "#### knn\n",
    "- use standard scaler in pipeline\n",
    "- explore n_neighbors: [4, 16, 32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with pipeline and standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe_knn = Pipeline([\n",
    "    None\n",
    "    ])\n",
    "knn_grid = {\n",
    "    None\n",
    "    }\n",
    "knn_model = best_model(pipe_knn, knn_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f60d6",
   "metadata": {},
   "source": [
    "#### GNB, LDA, QDA\n",
    "- No pipeline needed\n",
    "    - **Discuss:** why Why doesn't it make that much sense to have a pipeline?\n",
    "- default parameters for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402db4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB, LDA, QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_model = None\n",
    "lda_model = None\n",
    "qda_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34183054",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "- use pca in pipeline\n",
    "    - n_components: [10, 20, 30]\n",
    "- explore \n",
    "    - criterion: ['gini', 'entropy']\n",
    "    - max_depth: [2, 3, 4,5]\n",
    "    - min_samples_split: [5, 10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_dt = Pipeline([\n",
    "    None\n",
    "    ])\n",
    "dt_grid = {\n",
    "    None\n",
    "    }\n",
    "dt_model = best_model(pipe_dt, dt_grid, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91070874",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "- use pca in pipeline\n",
    "    - n_components: [10, 20, 30]\n",
    "- explore\n",
    "    - n_estimators = [10, 50, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a24eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import decomposition\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    None\n",
    "    ])\n",
    "rf_grid = {\n",
    "    None\n",
    "    }\n",
    "rf_model = best_model(pipe_rf, rf_grid, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bb080",
   "metadata": {},
   "source": [
    "### 2.2 Compare models\n",
    "**Task:**\n",
    "- Which model has the best accuracy on test data? Print it.\n",
    "- What were the hyperparameters used in the best model's training? Can you recognize which are the hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "models = [knn_model, gnb_model, lda_model, qda_model, dt_model, rf_model]\n",
    "\n",
    "# Best model.\n",
    "model_accuracies = [acc(y_test, mod.predict(X_test)) for mod in models]\n",
    "print('models     :', '[knn_model, gnb_model, lda_model, qda_model, dt_model, rf_model]')\n",
    "print('accuracies :', np.round(model_accuracies, 4))\n",
    "\n",
    "best_model_idx = np.argmax(model_accuracies)\n",
    "print('\\n*best model:', best_model_idx+1)\n",
    "print(models[best_model_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bfc390-70d3-4dc9-8ba7-f386e7a7ede7",
   "metadata": {},
   "source": [
    "### 2.3 Remove standard scaler/PCA and rerun the cells above\n",
    "- Standard scaler rescales the data in each feature/dimension to have variance 1. \n",
    "- PCA reduces the dimensionality of the data.\n",
    "\n",
    "These are preprocessing steps in the data, though not necessarily the \"right\" thing to do. Try removing the standard scaler/PCA and see what effect it has on the prediction accuracy.\n",
    "- you can do this by simply commenting out relevant lines of code\n",
    "\n",
    "**Disuss:** What is the best model now? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13391647",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
