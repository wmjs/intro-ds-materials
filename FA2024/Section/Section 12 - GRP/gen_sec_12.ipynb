{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "A Gaussian Process (GP) is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. It is completely specified by its mean function $m(x)$ and covariance function $k(x, x')$:\n",
    "\n",
    "$f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$\n",
    "\n",
    "where:\n",
    "- $m(x)$ is the mean function (often assumed to be 0)\n",
    "- $k(x, x')$ is the kernel function defining the covariance\n",
    "\n",
    "One of the more commonly used kernels ($k$) is the RBF kernel:\n",
    "\n",
    "$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{||x - x'||^2}{2l^2}\\right)$\n",
    "\n",
    "where:\n",
    "- $\\sigma^2$ is the variance parameter\n",
    "- $l$ is the length scale parameter\n",
    "\n",
    "\n",
    "Given training data $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, the posterior distribution at a new point $x_*$ is:\n",
    "\n",
    "$p(f_* | x_*, \\mathcal{D}) = \\mathcal{N}(\\mu_*, \\sigma_*^2)$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mu_* = k_*^T(K + \\sigma_n^2I)^{-1}y$\n",
    "\n",
    "$\\sigma_*^2 = k_{**} - k_*^T(K + \\sigma_n^2I)^{-1}k_*$\n",
    "\n",
    "- $K$ is the kernel matrix between all training points\n",
    "- $k_*$ is the kernel vector between $x_*$ and all training points\n",
    "- $k_{**}$ is the kernel value between $x_*$ and itself\n",
    "- $\\sigma_n^2$ is the noise variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TinyGP\n",
    "\n",
    "For the first part of this notebook we will be using TinyGP to allow us to interact with the Gaussian Processes at a lower level.\n",
    "\n",
    "Make sure that you can run the following cell before continueing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygp import GaussianProcess\n",
    "from tinygp import kernels\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of Gaussian Process regression is the ability to work with differnt specialized kernels depending on the problem at hand. These kernels allow you to view the data from different perspectives to better understand it.\n",
    "\n",
    "## Example Kernel Functions and Their Properties\n",
    "\n",
    "### 1. Radial Basis Function (RBF) Kernel a.k.a Squared Exponential\n",
    "$k_{RBF}(x, x') = \\sigma^2 \\exp\\left(-\\frac{||x - x'||^2}{2l^2}\\right)$\n",
    "- Infinitely differentiable\n",
    "- Universal kernel\n",
    "- Suitable for smooth functions\n",
    "\n",
    "### 2. Mat√©rn Kernel\n",
    "$k_{Matern}(x, x') = \\sigma^2\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2\\nu}\\frac{||x-x'||}{l}\\right)^\\nu K_\\nu\\left(\\sqrt{2\\nu}\\frac{||x-x'||}{l}\\right)$\n",
    "- Controls smoothness through parameter $\\nu$\n",
    "- More realistic for physical processes\n",
    "\n",
    "### 3. Periodic Kernel\n",
    "$k_{Per}(x, x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi||x-x'||/p)}{l^2}\\right)$\n",
    "- Captures periodic patterns\n",
    "- Useful for seasonal data\n",
    "\n",
    "### 4. Cosine Kernel\n",
    "$k_{cos}(x,x') = \\cos (2 \\pi r)$\n",
    "\n",
    "\n",
    "Below are the plots of a few of these kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel(kernel, **kwargs):\n",
    "    dx = np.linspace(0, 5, 100)\n",
    "    plt.plot(dx, kernel(dx, dx[:1]), **kwargs)\n",
    "    plt.xlabel(\"dx\")\n",
    "    plt.ylabel(\"k(dx)\")\n",
    "\n",
    "kernel = kernels.ExpSquared(scale=1.5)\n",
    "plot_kernel(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernel(kernel, label=\"original\", ls=\"dashed\")\n",
    "\n",
    "kernel_scaled = 4.5 * kernels.ExpSquared(scale=1.5)\n",
    "plot_kernel(kernel_scaled, label=\"scaled\")\n",
    "\n",
    "kernel_sum = kernels.ExpSquared(scale=1.5) + 2 * kernels.Matern32(scale=2.5)\n",
    "plot_kernel(kernel_sum, label=\"sum\")\n",
    "\n",
    "kernel_prod = 2 * kernels.ExpSquared(scale=1.5) * kernels.Cosine(scale=2.5)\n",
    "plot_kernel(kernel_prod, label=\"product\")\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Different Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "kernel_rbf = # TODO: Squared Exponential\n",
    "plot_kernel(kernel_rbf, label=\"RBF / Squared Exponential\")\n",
    "\n",
    "kernel_matern =  # TODO: Matern\n",
    "plot_kernel(kernel_matern, label=\"Matern\")\n",
    "\n",
    "kernel_cosine =  # TODO: Cosine (might have to scale this one to have it fit nicely)\n",
    "plot_kernel(kernel_cosine, label=\"Cosine\")\n",
    "\n",
    "kernel_expsine =  # TODO: Exponential Sin (play around with some of the parameters)\n",
    "plot_kernel(kernel_expsine, label=\"Matern\")\n",
    "\n",
    "_ = plt.legend(loc = \"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A set of candidates\n",
    "\n",
    "Like in many other situations we are trying to fit a function to a set of data. Without having ALL of the data there are infinite possibilities for what the potential function is that generated our data. Gaussian Processes do a good job of creating a confidence interval for the candidate functions.\n",
    "\n",
    "In the cell below we create a sample dataset and plot some candidate functions using the squared exponential kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.sort(np.random.default_rng(1).uniform(0, 10, 100))\n",
    "gp = GaussianProcess(kernel, X)\n",
    "\n",
    "y = gp.sample(jax.random.PRNGKey(4), shape=(5,))\n",
    "plt.plot(X, y.T, color=\"k\", lw=0.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sampled observations\")\n",
    "_ = plt.title(\"exponential squared kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Kernels\n",
    "\n",
    "Another unique part of Gaussian Processes is that we can create custom easily by multiplying other kernels together. This allows us to be more flexible with our data.\n",
    "\n",
    "In this cell create a custom kernel by multipling an `ExpSquared` with length scale 1.5 and a `Cosine` kernel with a scale of 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_prod =  # TODO: Create a custom kernel through multiplication\n",
    "gp = GaussianProcess(kernel_prod, X, diag=1e-5)\n",
    "y = gp.sample(jax.random.PRNGKey(4), shape=(5,))\n",
    "\n",
    "plt.plot(X, y.T, color=\"k\", lw=0.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sampled observations\")\n",
    "_ = plt.title(\"product of kernels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the mean\n",
    "\n",
    "Once we have all these potential candidates, we can take the mean of them to determine the actual function that was used to generate our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A GP with a non-zero constant mean\n",
    "gp =  # TODO: Define a GP with a constant mean\n",
    "y_const =  # TODO: Sample from the GP\n",
    "\n",
    "# And a GP with a general mean function\n",
    "def mean_function(x):\n",
    "    return 5 * jax.numpy.sin(x)\n",
    "\n",
    "gp =  # TODO: Define a GP with a non-constant mean using the mean function defined.\n",
    "y_func = # TODO: Sample\n",
    "\n",
    "# Plotting these samples\n",
    "_, axes = plt.subplots(2, 1, sharex=True)\n",
    "ax = axes[0]\n",
    "ax.plot(X, y_const.T, color=\"k\", lw=0.5)\n",
    "ax.axhline(2.0)\n",
    "ax.set_ylabel(\"constant mean\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(X, y_func.T, color=\"k\", lw=0.5)\n",
    "ax.plot(X, jax.vmap(mean_function)(X), label=\"mean\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"x\")\n",
    "_ = ax.set_ylabel(\"mean function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distribution of the candidate functions\n",
    "\n",
    "Because we have created this distribution of candidate functions we can sample from it and calculate different relevant statistics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a made up dataset, as an example\n",
    "random = np.random.default_rng(1)\n",
    "X = np.sort(random.uniform(0, 10, 10))\n",
    "y = np.sin(X) + 1e-4 * random.normal(size=X.shape)\n",
    "\n",
    "# Compute the log probability\n",
    "kernel = 0.5 * kernels.ExpSquared(scale=1.0) # Define a kernel\n",
    "gp = GaussianProcess(kernel, X, diag=1e-4) \n",
    "print(gp.log_probability(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the whole thing\n",
    "\n",
    "Below we visualize all these steps in one plot. You can see the confidence bounds of the distribution describing our candidate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(0, 10, 100)\n",
    "_, cond_gp = gp.condition(y, X_test)\n",
    "\n",
    "# The GP object keeps track of its mean and variance, which we can use for plotting confidence intervals\n",
    "\n",
    "mu = # TODO: Extract mean from conditioned gp\n",
    "std =  # TODO Extract std\n",
    "\n",
    "plt.plot(X_test, mu, \"C1\", color = \"blue\", label=\"mean\")\n",
    "plt.plot(X_test, mu + std, \"--C1\", label=\"1-sigma region\")\n",
    "plt.plot(X_test, mu - std, \"--C1\")\n",
    "\n",
    "# We can also plot samples from the conditional\n",
    "y_samp = cond_gp.sample(jax.random.PRNGKey(1), shape=(12,))\n",
    "plt.plot(X_test, y_samp[0], \"C0\", lw=0.5, alpha=0.5, label=\"samples\")\n",
    "plt.plot(X_test, y_samp[1:].T, \"C0\", lw=0.5, alpha=0.5)\n",
    "\n",
    "plt.plot(X, y, \".k\", label=\"data\")\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlim(X_test.min(), X_test.max())\n",
    "plt.xlabel(\"x\")\n",
    "_ = plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression and real world science\n",
    "\n",
    "Guassian process regression fits very well with the idea of efficient experimentation. You want to want to be able to gain as much possible information with as few trials as possible. To do this you need to be able to know what you don't know. A question that is usually very difficult to answer. With GPR, you are able to calculate your confidence intervals of the distribution of candidate functions that may fit your data. With this information you are able to perform experiments that maximize imformatian gain from trial to trial.\n",
    "\n",
    "In the example below assume the true function is \n",
    "- $y = \\sin(x) + \\frac{1}{5}\\cos(x)$\n",
    "\n",
    "Additionally assume that in this experiement there is a sample noise:\n",
    "- $\\epsilon \\sim N(0,\\frac{1}{10})$\n",
    "\n",
    "Then the sampled data would have a function of \n",
    "- $y = \\sin(x) + \\frac{1}{5}\\cos(x) + \\epsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_true_function(x):\n",
    "    \"\"\"Generate a nonlinear function for demonstration\"\"\"\n",
    "    return  # TODO: True function\n",
    "\n",
    "def generate_function_with_noise(x):\n",
    "    return  # TODO: Sampling function\n",
    "\n",
    "def plot_gpr_progression(X_train, y_train, kernel, x_test):\n",
    "    \"\"\"Plot GPR predictions with confidence intervals\"\"\"\n",
    "    gpr =  # TODO: Instantiate GPR with kernel = kernel and random_state = 42\n",
    "     # TODO: Fit the GPR\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred, sigma =  # TODO: Get y_pred and sigma (predictions and std) from gpr.predict\n",
    "    \n",
    "    # Plot true function\n",
    "    plt.plot(x_test, generate_true_function(x_test), 'k--', label='True Function')\n",
    "    \n",
    "    # Plot training points\n",
    "    plt.scatter(X_train, y_train, color='red', label='Training Points')\n",
    "    \n",
    "    # Plot prediction and confidence intervals\n",
    "    plt.plot(x_test, y_pred, 'b-', label='GPR Prediction')\n",
    "    plt.fill_between(x_test, y_pred - 2*sigma, y_pred + 2*sigma, \n",
    "                     color='blue', alpha=0.2, label='95% Confidence Interval')\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title(f'GPR with {len(X_train)} Training Points')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data with different numbers of observed data points and see how our distribution changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Generate test points for smooth visualization\n",
    "x_test = np.linspace(0, 5, 200)\n",
    "\n",
    "# Define kernel \n",
    "kernel = RBF(length_scale=1.0) # TODO: Define Kernel\n",
    "\n",
    "# Small training set\n",
    "X_small =  # TODO: Define an array of 3 x-values within the range of x_test\n",
    "y_small = generate_true_function(X_small)\n",
    "\n",
    "# Medium training set\n",
    "X_medium = # TODO: Define an array of 5 x-values within the range of x_test\n",
    "y_medium = generate_true_function(X_medium)\n",
    "\n",
    "# Large training set\n",
    "X_large =  # TODO: Define an array of 8 x-values within the range of x_test\n",
    "y_large = generate_true_function(X_large)\n",
    "\n",
    "plot_gpr_progression(X_small, y_small, kernel, x_test)\n",
    "\n",
    "plot_gpr_progression(X_medium, y_medium, kernel, x_test)\n",
    "\n",
    "plot_gpr_progression(X_large, y_large, kernel, x_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
