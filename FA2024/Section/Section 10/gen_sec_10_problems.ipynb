{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10 - Clustering Models: GMM, Spectral Embedding and Clustering\n",
    "\n",
    "This section we're going to:\n",
    "\n",
    "- Review lecture content on clustering methods;\n",
    "- Better understand Spectral Embedding model;\n",
    "- Hands on experience using this clustering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models (GMMs)\n",
    "\n",
    "### Overview\n",
    "Gaussian Mixture Models (GMMs) are a probabilistic model that assumes all data points are generated from a mixture of Gaussian distributions with unknown parameters. We'll explore:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y_true = make_blobs(n_samples=500, centers=3, cluster_std=1, random_state=42)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y_true, s=40)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend(*scatter.legend_elements())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit a GMM with 3 components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points and Gaussian components\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "x, y = np.meshgrid(np.linspace(-10, 10, 100), np.linspace(-10, 15, 100))\n",
    "XX = np.array([x.ravel(), y.ravel()]).T\n",
    "Z = -gmm.score_samples(XX)\n",
    "Z = Z.reshape(x.shape)\n",
    "plt.contour(x, y, Z, levels=10, zorder=1)\n",
    "plt.title(\"Gaussian Components and Contour Levels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=300, noise=0.07, random_state=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Recap\n",
    "\n",
    "### 1a. Big Idea\n",
    "- Spectral Embedding is a **<u>dimensionality reduction technique</u>** that lies within manifold learning methods. \n",
    "- The central idea for those methods is that, even though the data may be represented in a high-dimensional space, the important patterns/characteristics of the data is actually **inherently lower dimensional**.\n",
    "\n",
    "### 1b. Similarity to PCA\n",
    "- Like PCA, if we can **<u>\"project\" our data onto some appropriate low-dimensional space</u>** (and using <u>eigen-magic!</u>), then we can perform tasks like classification or clustering to this new (and hopefully simpler) representation of the data. \n",
    "- Unlike PCA, Spectral Embedding uses the <u>**graph/network of the data**</u>, instead of the covariance of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Spectral Embedding Method\n",
    "1. **Adjacency A.** Construct the graph adjacency matrix $A$, which has dimension $n$ by $n$, \"number of data points\" by \"number of data points\". \n",
    "    - Vertices are connected $A_{ij} = A_{ji} = 1$ if and only if data points are \"close enough\" $\\|x_i-x_j\\| < d_T$ some threshold distance.\n",
    "2. **Degree D.** Compute graph degree matrix $D$, where diagonal entries are degree of each vertex. \n",
    "3. **Laplacian L.** Compute graph Laplacian $L=D-A$. \n",
    "4. **Eigendecomposition.** Perform an eigendecomposition $L = U \\Lambda U^T$ to get the eigenvectors and eigenvalues, and study those objects to get a sense of the data in a new basis.\n",
    "\n",
    "### 2b. Spectral Clustering Method\n",
    "- After embedding, we can perform clustering on the eigenvectors using any of your favorite clustering technique. (manually thresholding, k-means, gmm, etc.)\n",
    "- Like PCA, we can consider muliple eigenvectors (\"multiple PCs\") when performing our clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Spectral embedding\n",
    "**Task:**\n",
    "- complete the code below steps 1-4 of spectral embedding\n",
    "- tweak the parameters where prompted to see how they affect the clustering assignment\n",
    "\n",
    "#### Step 1: Adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the adjacency matrix\n",
    "sqd_residual = (X[np.newaxis, :, :] - X[:, np.newaxis, :]) ** 2\n",
    "dist = None         # TODO\n",
    "\n",
    "## Tweak the epsilon param between 0.1 to 1\n",
    "## see how this affects clustering\n",
    "## e.g. 0.1, 0.3, 0.6, 1\n",
    "epsilon = None      # TODO\n",
    "A = None            # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:  Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree matrix\n",
    "degree = None       # TODO\n",
    "D      = None       # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3:  Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the graph Laplacian\n",
    "L = None            # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4:  Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigendecomposition\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try using other evecs, such as 3rd and 4th lowest,\n",
    "## to see how that affects the clustering assignment\n",
    "evec_num = None       # TODO\n",
    "evec     = None       # TODO\n",
    "s = np.argsort(evec)\n",
    "\n",
    "plt.plot(evec[s], 'x', alpha=0.5)\n",
    "plt.title('Sorted eigenvector %d' % evec_num)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "### Option 1: heuristic/round-off/manual on evec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering data\n",
    "label_a = (evec < 0).astype(int)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.suptitle('epsilon = %.1f,   evec %d' % (epsilon, evec_num))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:,0], X[:,1], c=label_a)\n",
    "plt.title(\"Clusters via spectral embedding + thresholding\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(np.arange(len(evec)), evec[s], marker='x', c=label_a[s], alpha=0.5)\n",
    "plt.hlines(0,0,300,color='k',alpha=0.4,linewidth=5, label='threshold')\n",
    "plt.title('Sorted evec %d, with labels' % evec_num)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: k-means, gmm, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, n_init=100, init='random')\n",
    "kmeans.fit(evec.reshape(-1,1))\n",
    "label_b = kmeans.labels_       # can negate with ~kmeans.labels_ for coloring purposes too.\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.suptitle('epsilon = %.1f,   evec %d' % (epsilon, evec_num))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=label_b)\n",
    "plt.title(\"Clusters via spectral embedding + k-means\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(np.arange(len(evec)), evec[s], marker='x', c=label_b[s], alpha=0.5)\n",
    "plt.scatter([0,0],centers,c='k',s=200,alpha=0.4,edgecolor='none', label='k-means centroid (1d)')\n",
    "plt.hlines(centers,0,300,color='k')\n",
    "plt.title('Sorted evec %d, with labels' % evec_num)\n",
    "plt.legend(loc='center left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - K-Means\n",
    "Compare spectral clustering against k-means.\n",
    "\n",
    "One can notice that Spectral Clustering groups the datapoints better than K-Means, which was expected, given the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=2, n_init=100, init='random').fit(X)\n",
    "label_c = ~kmeans_model.labels_      # Try negating the labels: ~kmeans_model.labels_\n",
    "C = kmeans_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=label_c) \n",
    "plt.scatter(C[:,0],C[:,1],c='k',s=300,alpha=0.4,edgecolor='none')\n",
    "plt.title(\"Clusters via k-means only\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
