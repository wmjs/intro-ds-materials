{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1aa7201",
   "metadata": {},
   "source": [
    "# Homework 5 - Naive Bayes\n",
    "\n",
    "Make sure you have downloaded:\n",
    "- heart_processed_log.csv\n",
    "\n",
    "This homework will ask you to implement naive bayes using a custom likelihood and then comparing it against a custom build LDA and QDA implimentation. \n",
    "\n",
    "The execution of GNB is slightly different from lecture and section. \n",
    "- It is more streamlined to take adavantage of vector multiplications and numpy functions, which has its own benefits if we want to scale up our naive bayes prediction to higher dimensions. \n",
    "- However, you may need to familiarize yourself with the \"dictionary\" data structure.\n",
    "\n",
    "Before attempting this homework, make sure you understand the broad strokes of naive Bayes. This will make your coding and debugging much smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d2714",
   "metadata": {},
   "source": [
    "## 0 Data\n",
    "Load `heart_processed.csv` from the [Heart Failure Clinical Records Dataset](https://archive.ics.uci.edu/ml/datasets/Heart%2Bfailure%2Bclinical%2Brecords)  It contains various predictors (which are in log-scale) for predicting the event of death `DEATH_EVENT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a787dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c6d33",
   "metadata": {},
   "source": [
    "Before submitting your homework, remember to set:\n",
    "- random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f648bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"heart_processed_log.csv\", index_col=0)\n",
    "display(dataset)\n",
    "\n",
    "X = dataset.drop(\"DEATH_EVENT\", axis=1).values\n",
    "y = dataset[\"DEATH_EVENT\"].values\n",
    "\n",
    "# split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# print the shapes of the training and testing sets\n",
    "print('train shapes:')\n",
    "print('\\t X_train ->', X_train.shape)\n",
    "print('\\t y_train ->', y_train.shape)\n",
    "\n",
    "print('test shapes:')\n",
    "print('\\t X_test ->', X_test.shape)\n",
    "print('\\t y_test ->', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9531e",
   "metadata": {},
   "source": [
    "Recall: naive Bayes is choosing the class $k$, $C_k$, that maximizes the posterior\n",
    "$$\n",
    "P(C_k \\lvert\\,{x}) = \\frac{\\pi(C_k)\\,{\\cal{}L}_{\\!{x}}(C_k)}{Z}.\n",
    "$$\n",
    "Hence, we maximize the numerator + assume that all $d$ features $x_i$ are independent (\"naive-ness\"). So we want to find the $k$ that satisfies\n",
    "$$\n",
    "\\max_k \\, \\pi(C_k)\\,{\\cal{}L}_{\\!{x}}(C_k) \\quad = \\quad \\max_k \\, \\left( \\pi(C_k)\\,\\prod_{i=1}^d p(x_i \\lvert C_k) \\right).\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca4be0c2",
   "metadata": {},
   "source": [
    "## 1 Custom Naive Bayes Classifier with KDE\n",
    "You will create a naive Bayes classifier:\n",
    "- using the training data\n",
    "- with KDE to approximate the likelihood\n",
    "- with bernoulli as the prior\n",
    "\n",
    "**Use only the training data ```X_train, y_train``` to fit the naive Bayes classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980cca8",
   "metadata": {},
   "source": [
    "### 1.1 Prior\n",
    "1. [2 pt] Compute ```prior```, a two element array. \n",
    "    - prior[0] is the probability of death event 0, $\\pi(C_0)$\n",
    "    - prior[1] is the probability of death event 1, $\\pi(C_1)$ \n",
    "    - You should construct the prior probabilities based on frequency of death events from the training data. \n",
    "    - Tip: Use np.unique() with return_counts.\n",
    "2. [1 pt] Print ```prior```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = None      # TODO\n",
    "\n",
    "print('The prior probabilities are:', prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeefe43",
   "metadata": {},
   "source": [
    "### 1.2 Likelihood (KDE)\n",
    "1. [2 pt] Define dictionaries `kde0` and `kde1` which fulfill the following:\n",
    "    - kde0[i] corresponds to the kde object (created by calling `scipy.stats.gaussian_kde`) for feature i when death event is 0. kde1[i] defined likewise.\n",
    "    - Make sure you index the correct rows of `X_train` when defining kdes.\n",
    "    - Use bandwidth method 'scott'. (For fun, you can try 'silverman' and see what difference in result you get.)\n",
    "    - As with all arrays you throw into sklearn or scipy, you may need to take transposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2818202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "kde0 = {} \n",
    "kde1 = {} \n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    kde0[i] = None # TODO\n",
    "    kde1[i] = None # TODO\n",
    "\n",
    "# display(kde1) # Use this to check what you made. swap kde0 for kde1 if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650786a",
   "metadata": {},
   "source": [
    "2. [2 pt] Complete the code for ```compute_likelihood``` function.\n",
    "    - The objects kde0[i] and kde1[i] have a method .pdf(), which you will use when computing the likelihood.\n",
    "        - Read the documentation to understand how it works.\n",
    "    - `likelihood0[j]` is the likelihood of seeing $j$ th data ${x_j} = \\left({x_j}_1, \\dots, {x_j}_d\\right)$ for death event 0, i.e., ${L}_{{x_j}}(C_0) = \\prod_{i=1}^d p({x_j}_i | C_0)$\n",
    "    - `likelihood1[j]` defined likewise.\n",
    "    - You can loop over the kde objects kde[i] to populate the likelihood arrays.\n",
    "    - Be careful with the shape of arrays. Print shapes as necessary when debugging.\n",
    "\n",
    "(Your solution shouldn't be very complicated. A working solutions needs only about 5-10 lines of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6afc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood(x, kde0, kde1):\n",
    "    # input:    x, a (# data) by (# features) array of test data\n",
    "    #           kde0 and kde1, dictionaries that will be used to compute the likelihood\n",
    "    # output:   likelihood, a (# data) by (# classes) array. \n",
    "    #           likelihood[j,k] is the likelihood of data j given class k\n",
    "    \n",
    "    # likelihood0[j] is the likelihood of data j given class 0. Analogously for likelihood1\n",
    "    likelihood0 = np.ones(x.shape[0])    \n",
    "    likelihood1 = np.ones(x.shape[0])    \n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        likelihood0 *= None\n",
    "        likelihood1 *= None\n",
    "\n",
    "    likelihood = np.vstack((likelihood0, likelihood1)).T\n",
    "    \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf5b7a",
   "metadata": {},
   "source": [
    "### 1.3 Posterior\n",
    "1. [2 pt] Complete the code for ```compute_posterior``` function. \n",
    "    - It should include calling the function ```compute_likelihood```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(x, prior, kde0, kde1):\n",
    "    # input:    x, a (# data) by (# features) array of test data\n",
    "    #           prior, a 1 by 2 array\n",
    "    #           kde0 and kde1, kde dictionaries that will be used to compute the likelihood\n",
    "    # output:   posterior, a (# data) by (# classes) array\n",
    "\n",
    "    likelihood = None # TODO\n",
    "    posterior = None # TODO\n",
    "    \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4126c3",
   "metadata": {},
   "source": [
    "### 1.4 Combine prior, likelihood, posterior\n",
    "Now, we are ready to piece all the code we prepared above.\n",
    "1. [2 pt] Complete the code for ```naive_bayes_predict```.\n",
    "    - Your code should include calling the ```compute_posterior``` function.\n",
    "    - Computing y_pred should be a simple one line of code. You may consider using numpy functions that find the index of the largest entry on every row.\n",
    "2. [1 pt] Complete the code for ```print_success_rates```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "362e2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(x, prior, kde0, kde1):\n",
    "    # input:    x, a (# data) by (# features) array\n",
    "    #           prior, a 1 by 2 array\n",
    "    #           kde0 and kde1, kde dictionaries that will be used to compute the likelihood\n",
    "    # output:   y_pred, an array of length (# data)\n",
    "\n",
    "    posterior = None # TODO\n",
    "    y_pred = None # TODO\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def print_success_rates(y_true,y_pred):\n",
    "    n_success = None   # TODO\n",
    "    n_total   = None    # TODO\n",
    "    print(\"Number of correctly labeled points: %d of %d.  Accuracy: %.2f\" \n",
    "        % (n_success, n_total, n_success/n_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39a565",
   "metadata": {},
   "source": [
    "### 1.5 Predict\n",
    "1. [1 pt] Use your custom naive Bayes to:\n",
    "    - predict *TRAINING* \n",
    "    - print the results with ```print_success_rates```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b05d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO predict training data and print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489ddcd",
   "metadata": {},
   "source": [
    "2. [1 pt] Use your custom naive Bayes to:\n",
    "    - predict *TEST* data\n",
    "    - print the results with ```print_success_rates```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e984d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO predict test data and print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457e2dd",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "### 1.6 random_state = 0\n",
    "Using random_state=0 and respond to the following questions.\n",
    "\n",
    "[2 pt] For **custom NB**, what is the difference between the training and test accuracy? Give an explanation for why it might be so.\n",
    "    \n",
    "**Ans:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2338897",
   "metadata": {},
   "source": [
    "### 1.7 change random_state\n",
    "Now, experiment with a range of random_state and respond to the following question.\n",
    "\n",
    "[2 pt] Does your responses to 3.1 change? If so, describe how your responses change and why you changed them.\n",
    "- (You do not need to artificially adjust your response to 3.1 to fit the any new findings you made after changing random_state)\n",
    "\n",
    "**Ans:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b427ae4",
   "metadata": {},
   "source": [
    "# 2 LDA and QDA\n",
    "\n",
    "In this section you will demonstrate your understanding of LDA and QDA by completing the following functions. But first a quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9e6d8",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### 2.1 \n",
    "\n",
    "[2 pt] What is the main difference between the assumptions of GNB, LDA, and QDA? Explain.\n",
    "\n",
    "**Ans**  \n",
    "\n",
    "#### 2.2 \n",
    "\n",
    "[2 pt] Which method is the most computational expensive? Explain.\n",
    "\n",
    "**Ans** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5cd60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118ca7b",
   "metadata": {},
   "source": [
    "### 2.3 LDA Implementation\n",
    "\n",
    "Complete the following code block to implement the `lda_predict` function. Most of the variables have already been named you just need to assign them.\n",
    "\n",
    "**Task**\n",
    "\n",
    "- [1 pt] Calculate prior of each class assuming a uniform prior\n",
    "- [$\\frac{1}{2}$ pt] Split the training data into two seperate class specific datasets\n",
    "- [1 pt] Calculate the mean of each class using `np.mean`\n",
    "- [1 pt] Calculate the covariance matrix for each class using `np.cov`\n",
    "- [1 pt] Calculate the likelihoods of each class using `multivariate_normal.pdf()`\n",
    "- [1 pt] Calculate the posterior for each class\n",
    "- [$\\frac{1}{2}$ pt] Return the predicted classifications\n",
    "\n",
    "HINTS:\n",
    "- Be careful with transposes and axis declarations \n",
    "    - Make sure to check the shapes of your calculated matrices to make sure they make sense in the context of the problem\n",
    "- `np.where` might be helpful... https://numpy.org/doc/2.0/reference/generated/numpy.where.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28ec95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_predict(X_train, y_train, X_test):\n",
    "    \n",
    "    prior_class_0 = None # TODO: prior likelihood of class 0\n",
    "    prior_class_1 = None # TODO: prior likelihood of class 1\n",
    "\n",
    "    X_class_0 = None # TODO: Seperate X_train by class\n",
    "    X_class_1 = None # TODO:\n",
    "    \n",
    "    mu_class_0 = None # TODO: Mean\n",
    "    mu_class_1 = None # TODO: Mean\n",
    "\n",
    "    sigma_class_0 = None # TODO: Proper covariance matrix for LDA\n",
    "    sigma_class_1 = None # TODO: Proper covariance matrix for LDA\n",
    "\n",
    "    likelihood_class_0 = None # TODO: Calculate likelihood\n",
    "    likelihood_class_1 = None # TODO: Calculate likelihood\n",
    "\n",
    "    posterior_class_0 = None # TODO: Posterior \n",
    "    posterior_class_1 = None # TODO: Posterior\n",
    "\n",
    "    return None # TODO: return predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168eb73e",
   "metadata": {},
   "source": [
    "### 2.4 QDA Implementation\n",
    "\n",
    "Complete the following code block to implement the `qda_predict` function. Most of the variables have already been named you just need to assign them. You can repeat much of the same code from LDA. Just don't get too overzealous and forget something ;)\n",
    "\n",
    "**Task**\n",
    "\n",
    "- [1 pt] Calculate prior of each class assuming a uniform prior\n",
    "- [$\\frac{1}{2}$ pt] Split the training data into two seperate class specific datasets\n",
    "- [1 pt] Calculate the mean of each class using `np.mean`\n",
    "- [1 pt] Calculate the covariance matrix for each class using `np.cov`\n",
    "- [1 pt] Calculate the likelihoods of each class `multivariate_normal.pdf()`\n",
    "- [1 pt] Calculate the posterior for each class\n",
    "- [$\\frac{1}{2}$ pt] Return the predicted classifications\n",
    "\n",
    "HINTS:\n",
    "- Be careful with transposes and axis declarations \n",
    "    - Make sure to check the shapes of your calculated matrices to make sure they make sense in the context of the problem\n",
    "- `np.where` might be helpful... https://numpy.org/doc/2.0/reference/generated/numpy.where.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ddceeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qda_predict(X_train, y_train, X_test):\n",
    "    prior_class_0 = None # TODO: prior likelihood of class 0\n",
    "    prior_class_1 = None # TODO: prior likelihood of class 1\n",
    "\n",
    "    X_class_0 = None # TODO: Seperate X_train by class\n",
    "    X_class_1 = None # TODO:\n",
    "    \n",
    "    mu_class_0 = None # TODO: Mean\n",
    "    mu_class_1 = None # TODO: Mean\n",
    "\n",
    "    sigma_class_0 = None # TODO: Proper covariance matrix for QDA\n",
    "    sigma_class_1 = None # TODO: Proper covariance matrix for QDA\n",
    "\n",
    "    likelihood_class_0 = None # TODO: Calculate likelihood\n",
    "    likelihood_class_1 = None # TODO: Calculate likelihood\n",
    "\n",
    "    posterior_class_0 = None # TODO: Posterior \n",
    "    posterior_class_1 = None # TODO: Posterior\n",
    "\n",
    "    return None # TODO: return predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef44208",
   "metadata": {},
   "source": [
    "### 2.5 Compare\n",
    "\n",
    "**Task**\n",
    "\n",
    "- [1 pt] Assign your predicted y values to `y_pred_lda` and `y_pred_qda`\n",
    "- [1 pt] Print their success rates with the `print_success_rates` from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lda = None # TODO\n",
    "y_pred_qda = None # TODO\n",
    "\n",
    "# TODO: Print sucess rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d08dc",
   "metadata": {},
   "source": [
    "### 2.6 Discuss\n",
    "\n",
    "[2 pt] Note the success rates of the two methods. Are they the same or different? If they are the same does that mean that they had the exact same predictions for each sample? Use the code block below to support your answer.\n",
    "\n",
    "**Ans** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Code for 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1fe16",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Before submitting your hw, set train test split to random_state=0. Restart kernel and rerun all cells. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
