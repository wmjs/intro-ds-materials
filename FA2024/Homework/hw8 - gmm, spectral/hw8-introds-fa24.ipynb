{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Cluster with GMM vs KMeans\n",
    "\n",
    "Homework 7 included clustering with K-Means. In this notebook we look at another clustering method in Gaussian Mixture Models.\n",
    "\n",
    "### Load in data\n",
    "\n",
    "Run the following cell to generate data for the notebook. The true seperation of the data is shown by the distinct coloring of points. Additionally, the true centers of the clusters has been plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### RUN THIS CELL DO NOT EDIT ############################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters for each Gaussian cluster\n",
    "np.random.seed(42)\n",
    "means = [[-4, 4], [4, 4], [0,4]]\n",
    "covariances = [[[1, 0], [0, 8]], [[1, 0], [0, 8]], [[1, 0], [0, 8]]]\n",
    "num_samples = 500\n",
    "\n",
    "# Generate data for each cluster\n",
    "data = []\n",
    "count = 0\n",
    "labels = []\n",
    "for mean, cov in zip(means, covariances):\n",
    "    cluster_data = np.random.multivariate_normal(mean, cov, num_samples)\n",
    "    count += 1\n",
    "    labels.append(np.ones(shape = cluster_data.shape[0]) * count)\n",
    "    data.append(cluster_data)\n",
    "\n",
    "# Combine data and plot\n",
    "data = np.vstack(data)\n",
    "plt.scatter(data[:, 0], data[:, 1], s=10, alpha=0.7, c = labels, cmap =\"cividis\")\n",
    "plt.scatter([mean[0] for mean in means], [mean[1] for mean in means], c='red', marker='X', label='True Means')\n",
    "plt.title(\"True Classes\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Clustering with K-Means\n",
    "\n",
    "Use the sklearn `Kmeans` package to cluster `data`\n",
    "\n",
    "**Tasks**\n",
    "- [1 pt] Import `KMeans` from sklearn\n",
    "- [1 pt] Create a classifier `km_clf` with 3 clusters and a `random_state` of 42\n",
    "- [1 pt] Assign predicted data to `km_clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Clustering with KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Plotting\n",
    "\n",
    "In this section you will need to use the clusters that you found in (1) to assign points to a cluster via the `c=` argument in the scatter plot. Additionally, extract the cluster centers from the KMeans classifier and plot those to see how they differ from the true cluster centers. Make sure to use a different color than the true centers to distinguish them. \n",
    "\n",
    "**Tasks** [2 pt]\n",
    "-  Plot the data with the KMeans clusters\n",
    "-  Plot the original centers (means) \n",
    "-  Extract the centroids from the KMeans classifier\n",
    "-  Plot the centroids in a different color on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_centroids = ... # TODO: Extract Centroids\n",
    "\n",
    "# TODO: Plot data with KMeans cluster assignments\n",
    "# TODO: Plot True Centers\n",
    "# TODO: Plot Centroids\n",
    "plt.title(\"KMeans Cluster Assignments\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Clustering with GMM\n",
    "\n",
    "Use the sklearn `GaussianMixture` package to cluster `data`\n",
    "\n",
    "**Tasks**\n",
    "- [1 pt] Import `GaussianMixture` from sklearn\n",
    "- [1 pt] Create a classifier `gmm_clf` with 3 clusters and a `random_state` of 42\n",
    "- [1 pt] Assign predicted data to `gmm_clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Clustering with GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Plotting\n",
    "\n",
    "In this section you will need to use the clusters that you found in (1) to assign points to a cluster via the `c=` argument in the scatter plot. Additionally, extract the cluster centers from the KMeans classifier and plot those to see how they differ from the true cluster centers. Make sure to use a different color than the true centers to distinguish them. \n",
    "\n",
    "**Tasks** [2 pt]\n",
    "-  Plot the data with the GMM clusters\n",
    "-  Plot the original centers (means) \n",
    "-  Extract the centers of the gaussians from the GMM classifier\n",
    "-  Plot these centers in a different color on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_means = ... # TODO: Extract Means\n",
    "\n",
    "# TODO: Plot data with GMM cluster assignments\n",
    "# TODO: Plot True Centers\n",
    "# TODO: Plot Means\n",
    "plt.title(\"GMM Cluster Assignments\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Discussion\n",
    "\n",
    "[2 pt] Which clustering method performed better? Explain.\n",
    "\n",
    "**Ans** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral embedding and clustering\n",
    "This homework will not only help you get a better sense of how spectral embedding and clustering work, but also give you a stronger intuition for the \"radial basis function\" kernel, which is a common kernel used in many scientific applications.\n",
    "\n",
    "\n",
    "Make sure you have downloaded:\n",
    "- X1.csv\n",
    "- X2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Intro\n",
    "\n",
    "### The following few markdowns are for your refrence, there is nothing to do until (2)\n",
    "\n",
    "Remember: \n",
    "- when performing spectral clustering, there is an adjacency matrix, which represents the graph/network of all data points. \n",
    "- Data points are connected if they are \"close enough\" to each together.\n",
    "- **Question:** But what constitutes close enough?\n",
    "- There are two ways to do this, as seen in the lecture notes on Embedding:\n",
    "    - k-nearest neighbors\n",
    "    - $\\exp\\left(-d^2/\\epsilon\\right)$ similarity: <u>radial basis function (RBF)</u> proximity.\n",
    "\n",
    "You know how k-nearest neighbors works. So this homework will start with introducing basic understanding of RBF and how it measures proximity. After that, we will make some functions that streamlines your code and experiment with spectral clustering models on synthetically generated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Function (RBF)\n",
    "The RBF kernel, a.k.a. squared exponential/Gaussian kernel, is a function that takes in two points and outputs a number to reflect the proximity of those two points.\n",
    "\n",
    "More formally, if the two points are $x_1$ and $x_2$, the \"proximity\" of these points are \n",
    "$$\n",
    "k(x_1, x_2) \\ := \\ \\exp\\left( - \\frac{\\|x_1-x_2\\|^2}{2\\ell^2}\\right),\n",
    "$$\n",
    "where $\\ell$ is a lengthscale parameter you can choose. \n",
    "\n",
    "($\\ell$ _essentially_ corresponds to the $\\epsilon$ in the lecture notes and should remind you of the variance of a normal distribution PDF.)\n",
    "\n",
    "The next task is to code up plot for RBF kernel, both in 1d and 2d. \n",
    "\n",
    "But before we do that, we make a simplifying trick to make these plots easier. \n",
    "- Notice that we are only concerned with the **distance** between two points, as represented in the $\\|x_1-x_2\\|$ term in the formula above.\n",
    "- Hence, we can set distance $d = \\|x_1-x_2\\|$ to get a new formula for RBF kernel:\n",
    "    $$\n",
    "    k(d) \\ := \\ \\exp\\left( - \\frac{d^2}{2\\ell^2}\\right),\n",
    "    $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(d,l):\n",
    "    return np.exp(-d**2 / (2*l**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,1000)\n",
    "l_list = [1,2,3]\n",
    "\n",
    "for l in l_list:\n",
    "    dist_1d = rbf(np.abs(x),l)\n",
    "    plt.plot(x,dist_1d,label=r'$\\ell$=%d'%l)\n",
    "plt.title('RBF kernel 1D')\n",
    "plt.xlabel('displacement from 0, d'); plt.ylabel(r'RBF, $k(d)$')\n",
    "plt.legend(); plt.grid(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(x, x)\n",
    "\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "plt.suptitle('RBF kernel 2D')\n",
    "for i in np.arange(len(l_list)):\n",
    "    l = l_list[i]\n",
    "    ax = fig.add_subplot(1,3,i+1,projection='3d')\n",
    "    dist_2d = rbf(np.sqrt((xx**2 + yy**2)),l)\n",
    "    ax.plot_surface(xx,yy,dist_2d)\n",
    "    ax.set_title(r'$\\ell$=%d'%l)\n",
    "    ax.set_xlabel(r'$x$'); ax.set_ylabel(r'$y$'); ax.set_zlabel(r'RBF, $k(d)$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Spectral embedding/clustering functions\n",
    "Before completing the spectral clustering experiments, complete the functions below.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "[3 pt] complete code below for `embed_and_plot()`\n",
    "- arguments, which will all be fed into the sklearn.manifold.SpectralEmbedding object:\n",
    "    - `X`, data, number of samples by number of dimensions\n",
    "    - `aff`, 'rbf' or 'nearest_neighbors' (short for affinity argument in SpectralEmbedding)\n",
    "    - `gam`, gamma parameter for RBF kernel\n",
    "    - `num_neighbors`, parameter for k-nearest neighbors\n",
    "    - You can set default values for gam and num_neigbors to make your function calls later easier. Here is an [example](https://www.geeksforgeeks.org/default-arguments-in-python/) of how to write default parameters.\n",
    "- your code should perform two things:\n",
    "    1. make SpectralEmbedding object, fit it to data X, and obtain eigenvectors of embedding.\n",
    "    2. display three plots in the same row.\n",
    "        - scatter plot of embedding coordinates (eigenvector 1 against eigenvector 2)\n",
    "        - scatter plot of embedding coordinates (eigenvector 2 against eigenvector 3)\n",
    "        - scatter plot of sorted entries of eigenvector 1 (sorted index i against entry i)\n",
    "- as usual, your plots should contain appropriate titles, axis labels, etc.\n",
    "- You should read the [SpectralEmbedding documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html) and [SpectralClustering documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) **carefully**, so as to not make mistakes when implementing. Useful skill for any coding activity.\n",
    "    - In particular, pay attention to their definition of $\\gamma$ compared to the definition of $\\ell$ in the earlier RBF kernel exercise. What is the inverse relationship between $\\gamma$ and $\\ell$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "def embed_and_plot(X, aff, gam=1, num_neighbors=1):\n",
    "    # TODO make model, fit, and obtain eigenvectors\n",
    "    \n",
    "    plt.figure(figsize=(18,4))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    # TODO scatter eigenvector 1 entries against eigenvector 2 entries\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    # TODO scatter eigenvector 2 entries against eigenvector 3 entries\n",
    "\n",
    "    plt.subplot(133)\n",
    "    # TODO scatter sorted eigenvector 1 entries\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "[2 pt] complete code below for `cluster_and_plot`\n",
    "- arguments:\n",
    "    - `X`, `aff`, `gam`, `num_neighbors` as in embed_and_plot\n",
    "    - `n_clus` is the number of clusters that you want to split the data X into.\n",
    "- your code should have two components:\n",
    "    1. make SpectralClustering object, fit to data X, and predict the cluster labels.\n",
    "    2. scatter plot of data with the predicted cluster labels. Include a title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def cluster_and_plot(X, n_clus, aff, gam=1, num_neighbors=1):\n",
    "    # TODO make model, fit, get cluster labels\n",
    "    \n",
    "    # TODO plot\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Cluster X1\n",
    "Load and scatter plot `X1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.loadtxt('X1.csv', delimiter=',')\n",
    "plt.scatter(X1[:,0], X1[:,1]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "[2 pt] call embed_and_plot and cluster_and_plot on X1.\n",
    "- you should correctly cluster the data into the two obvious clusters.\n",
    "- experiment with different choices of affinity (rbf/nearest_neighbors) and parameter values (gamma/n_neighbors). \n",
    "- But you only need to submit plots for one choice of affinity and parameter that successfully clusters the data.\n",
    "- all plots should be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Cluster X2\n",
    "Load and scatter plot `X2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.loadtxt('X2.csv', delimiter=',')\n",
    "plt.scatter(X2[:,0], X2[:,1]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 cluster\n",
    "[2 pt] call embed_and_plot and cluster_and_plot on X2.\n",
    "- same instructions as X1, except that you should obtain 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compare to k-means and GMM\n",
    "**Task:**\n",
    "\n",
    "1. [2 pt] Think back to other clustering algorithms in this course. Between k-means and GMM, which is more approapriate for X2 data? \n",
    "    - Give a reason why your chosen clustering algorithm is suited the data. \n",
    "    - Give a reason why clustering algorithm you did not choose has a limitation that prevents it from being suitable to the data.\n",
    "\n",
    "    **Ans:** \n",
    "\n",
    "2. [3 pt] Implement the clustering algorithm that you chose in the previous question to be appropriate for X2 data.\n",
    "    - You may use relevant sklearn packages.\n",
    "    - Include a plot of the cluster labels to ensure that it is working exactly to how you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement clustering based on your answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
