{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Basic Bayes\n",
    "You have seen a demostration of Bayesian inference in section. Your homework will explore simple variation of it to solidify your understanding of priors, likelihoods, and posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Bayes Rule Introduction and Warm Up (2 pts)\n",
    "\n",
    "Recall Bayes: $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "Imagine you are a dietician for toddlers. You are trying to recommend to parents which fruit they should buy their kids. You have a client 'Tommy' who likes bananas. You also know, from your many years as a toddler dietician, the following statistics:\n",
    "- If a child likes apples it is 95% gaurenteed that they also like bananas.\n",
    "- 50% of children like apples.\n",
    "- 75% of children like bananas.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "- [1 pt] Use Bayes Theorem to calculate the probability that Tommy likes apples\n",
    "- [1 pt] Print the probabilty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Translate probabilities\n",
    "p_likes_bananas_given_apples = None\n",
    "p_likes_apples = None\n",
    "p_likes_bananas = None\n",
    "\n",
    "# TODO Bayes theorem implementation \n",
    "\n",
    "\n",
    "# TODO Print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Bayesian Inference with Synthetic Data (17 pts)\n",
    "\n",
    "In this section, you will simulate a dataset and update a prior distribution using Bayes' Rule.\n",
    "\n",
    "Your friend Steve is a really sleazy guy. He wants to make a bet with you on the outcome of a coin being flipped. The only catch is that he wants to bet that his lucky \"totally fair\" coin will land on heads (a binomial success). Obviously you don't believe him and decide to run an experiment to understand the true fairness of the coin. \n",
    "\n",
    "You decide to flip the coin 100 times to get a good understanding of the bias of the coin. Additionally, knowing that Steve is completely unpredictable you have a prior belief that the coin is equally likley to be biased in any amount towards heads (i.e. your prior is a uniform distribution across the values 0-1)\n",
    "\n",
    "### 2.1 Setup\n",
    "\n",
    "**Tasks**\n",
    "- [1 pts] Set the `true_bias` to .7 and `n_flips` to 100\n",
    "- [2 pts] Use `np.random.binomial` to simulate the 100 flips. Save them in `flips`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Coin flip experiment setup\n",
    "true_bias = None\n",
    "n_flips = None\n",
    "flips = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize Prior\n",
    "\n",
    "For our own sake we are going to assume that for our prior distribution all values **UP TO 2 decimal places** within 0-1 (inclusive) are equally likely. \n",
    "\n",
    "That means that .01, .79, .54, etc... all equally likely. \n",
    "\n",
    "**Tasks**\n",
    "- [1 pts] Create a list of all possible bias values and store it in `bias_values`\n",
    "- [2 pts] Create the prior (uniform) distribution across all possible coin biases\n",
    "    - Save this in the variable `prior`\n",
    "    - You you output a numpy array of length 101\n",
    "    - Hint: You can create an array of ones and divide them all by the length of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize prior (uniform distribution over 101 possible biases)\n",
    "bias_values = None\n",
    "prior = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Function to update prior\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "- [3 pts] Fill in the missing code in the function below to update the prior distribution with more data\n",
    "    - The posterior returned should be an rray of length 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Function to update prior based on observed flip\n",
    "def update_posterior(prior, flip, bias_values):\n",
    "    likelihood = bias_values if flip == 1 else (1 - bias_values)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Updating prior\n",
    "\n",
    "**Tasks**\n",
    "- [2 pts] For each flip in `flips` update the `posterior` using the function defined above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform Bayesian updating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Plot\n",
    "\n",
    "**Task**\n",
    "- [2 pts] On **ONE** plot. Show the posterior distribution after 5, 10, 20, and 100 flips\n",
    "- [1 pts] Include appropriate axis titles and legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot posterior distribution after 5, 10, 20 and 100 flips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Discussion\n",
    "\n",
    "[1 pt] How does the prior influence the model's in early iterations (with little data) versus later iterations (with more data)?\n",
    "\n",
    "**Ans** Meaningful discussion about how the prior becomes less important as we gather more data\n",
    "\n",
    "[2 pt] Should you take the bet with Steve? Why or why not. Include reasoning drawn from 1.5\n",
    "\n",
    "**Ans** No. The plot of the distribution shows us that we are confident that the coin is biased to land on heads 70% of the time. This means that Steve is likely to win the bet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 Intro: Poisson Likelihood + Gamma Prior -> Gamma Posterior\n",
    "\n",
    "### Setup\n",
    "Assume we have $X_1, X_2, \\dots, X_n$ independent and identically distributed (i.i.d) Poisson distribution. So, \n",
    "\n",
    "$$X_i \\sim \\text{Pois}(\\lambda) \\text{ for all } i.$$\n",
    "\n",
    "You can imagine $x_i$ as counting the number of telephone calls in day $i$, which follows a Poisson distribution, where $\\lambda$ is the (unknown) average number of phone calls a day.\n",
    "\n",
    "**Goal:** We want to conduct Bayesian inference on the data $x_1, \\dots, x_n$ in order to infer the unknown parameter $\\lambda$.\n",
    "\n",
    "### a. Likelihood $p(x | \\lambda)$\n",
    "The pmf of Poisson is \n",
    "\n",
    "$$p(x | \\lambda) = \\frac{\\lambda^{x}e^{-\\lambda}}{x!}.$$\n",
    "\n",
    "Therefore, by i.i.d. assumption, the joint likelihood of all the $n$ pieces of data will be the product of the pmf, simplified for you here\n",
    "$$p (x_1, \\dots, x_n) = \\frac{\\lambda^{x_1 + \\dots  + x_n} e^{-n\\lambda}}{x_1! \\dots x_n!}.$$\n",
    "\n",
    "### b. Prior $p(\\lambda)$\n",
    "Remember, we don't know what $\\lambda$ is, so we will treat it as a random variable $\\Lambda$ (this is capital letter for $\\lambda$). Magically, if we let $\\Lambda$ follow a Gamma distribution, we get a nice posterior, so we will do just that. Now, the pdf of gamma(shape=$\\alpha$, rate=$\\beta$) is\n",
    "\n",
    "$$p(\\lambda) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda},$$\n",
    "\n",
    "where $\\Gamma(\\cdot)$ is the gamma function. When you choose your prior, **choose any $\\alpha >0, \\beta>0$ that is suitable for the prior knowledge you have about the data**. See section notes if you feel unfamiliar about it.\n",
    "\n",
    "_Do not be intimidated by this crazy formula! It will be very friendly to us in the end of the calculation. Question: doesn't the gamma distribution look kind of similar to the Poisson distribution? This may give us a sense that the posterior will be nice! :)_\n",
    "\n",
    "### c. Posterior $p(\\lambda | x)$\n",
    "We're almost done. \n",
    "\n",
    "Recall: The formula of the posterior distribution is\n",
    "$$\n",
    "p(\\lambda | x) = \\frac{p(x|\\lambda)p(\\lambda)}{\\int_{\\lambda} \\ p(x|\\lambda)p(\\lambda) \\ d\\lambda} = \\frac{\\text{likelihood} \\cdot \\text{prior}}{\\text{normalizing constant}}.\n",
    "$$\n",
    "We will skip the algebra and just tell you that \n",
    "$$\n",
    "p(\\lambda | x) = \\frac{\\lambda^{{\\color{red}{x_1 + \\dots + x_n + \\alpha}} - 1} e^{-({\\color{red}{n + \\beta}})\\lambda}}{\\text{normalizing constant}}.\n",
    "$$\n",
    "The normalizing constant is not very important, the **MAIN TAKEAWAY IS THAT THE POSTERIOR IS ALSO DISTRIBUTED GAMMA! :)** In fact, the posterior is \n",
    "$$\n",
    "\\text{gamma}(x_1 + \\dots + x_n + \\alpha, \\ n + \\beta).\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "- Likelihood $p(x | \\lambda) \\sim \\text{Pois}(\\lambda)$\n",
    "- Prior $p(\\lambda) \\sim \\text{gamma}(\\alpha, \\beta)$\n",
    "- Posterior $p(\\lambda | x)\\sim \\text{gamma}(x_1 + \\dots + x_n + \\alpha, \\ n + \\beta)$\n",
    "\n",
    "With this, you are ready to tackle the first problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Peaked Prior (13 pts)\n",
    "Run the following code cell.\n",
    "\n",
    "Let the full data ($n=30$) be\n",
    "$$\n",
    "x = [ 8,  8,  7, 11, 10,  6,  7, 11,  5, 12,  8,  7,  8,  8, 11,  4,  3,\n",
    "        9,  9,  4,  7,  7,  9, 12,  8,  9, 10,  9,  8,  8]\n",
    "$$\n",
    "Define:\n",
    "- ```x```, the original 30 data points\n",
    "- ```x_short```, only the first 3 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "x = [ 8,  8,  7, 11, 10,  6,  7, 11,  5, 12,  8,  7,  8,  8, 11,  4,  3,  9,  9,  4,  7,  7,  9, 12,  8,  9, 10,  9,  8,  8]\n",
    "n = len(x)\n",
    "print('n:', n, '\\nmean:', np.mean(x))\n",
    "x_short = x[:3]\n",
    "n_short = len(x_short)\n",
    "print('x_short:', x_short)\n",
    "print('n_short:', n_short, '\\nmean x_short:', np.round(np.mean(x_short),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prior\n",
    "\n",
    "Recall the story about inferring the average number of phone calls. First, we want to create a (peaked) prior that reflects our belief about what the data (number of phone calls) is.     \n",
    "\n",
    "Suppose Mrs. Morgan said, \"From my experience and memory, I think the average number of phone calls every day is 4. Most of the time (like 95% of the time), it's between 2 to 6 calls every day.\"\n",
    "\n",
    "\n",
    "**Task:**\n",
    "1. [2 pt] Define appropriate ```alpha_prior1```, ```beta_prior1``` to obtain a suitable prior gamma($\\alpha, \\beta$) with appropriate mean and variance. \n",
    "\n",
    "    (Hint: for gamma($\\alpha, \\beta$)\n",
    "    - mean = $\\frac{\\alpha}{\\beta}$ and variance = $\\frac{\\alpha}{\\beta^2}$. \n",
    "    - It may also be easier to define $\\beta$ before defining $\\alpha$.)\n",
    "2. [1 pt] Compute ```prior1```, which is the pdf of gamma($\\alpha, \\beta$) along the $\\lambda$-axis `lamb`. \n",
    "    - Make sure you read the scipy documentation correctly and input the correct arguments (rate and scale are reciprocals of each other!).\n",
    "3. [1 pt] Plot the density of the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO alpha and beta\n",
    "beta_prior1 = None\n",
    "alpha_prior1 = None\n",
    "\n",
    "# TODO prior1\n",
    "lamb = np.linspace(0, 15, 1000)\n",
    "prior1 = None\n",
    "\n",
    "# TODO plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Posterior parameters\n",
    "Let the posterior parameters be\n",
    "- ```alpha_post1```, be the posterior shape for full data ```x```.\n",
    "- ```beta_post1```, be the posterior rate for full data ```x```.\n",
    "- ```alpha_post1_short```, be the posterior shape for short data ```x_short```.\n",
    "- ```beta_post1_short```, be the posterior rate for short data ```x_short```.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "[2 pt] Define ```alpha_post1```, ```beta_post1```, ```alpha_post1_short```, and ```beta_post1_short``` using the formula in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO alpha, beta\n",
    "alpha_post1 = None\n",
    "beta_post1 = None\n",
    "\n",
    "alpha_post1_short = None\n",
    "beta_post1_short = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Posterior plot\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. [2 pt] Define ```posterior1``` and ```posterior1_short```, the respective pdf of the posteriors. \n",
    "    - Use the same horizontal axis ```lamb``` from previous parts.\n",
    "2. [1 pt] Plot the three densities (prior1, posterior1, posterior1_short) on the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO posterior densities\n",
    "posterior1 = None\n",
    "posterior1_short = None\n",
    "\n",
    "# TODO plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Peaked MAP\n",
    "**Task:**\n",
    "\n",
    "1. [1 pt] Compute the MAP estimator for both posteriors, storing it as ```lamb_MAP1``` and ```lamb_MAP1_short```. \n",
    "\n",
    "2. [1 pt] Print both these values, rounded to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO MAP\n",
    "lamb_MAP1 = None\n",
    "lamb_MAP1_short = None\n",
    "\n",
    "# TODO print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Discuss MAP\n",
    "[2 pt] What do you observe about the MAP estimator for the full data and short data? Which is \"closer\" to the prior? Give an explanation for your observations.\n",
    "\n",
    "**Ans:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
